#!/usr/bin/env python3
"""
æŸ¥çœ‹äº‘å­˜å‚¨ä¸­çš„é¢„æ ‡æ³¨ç»Ÿè®¡
æ‰«ææ‰€æœ‰ *_bbox.json æ–‡ä»¶ï¼Œç»Ÿè®¡æ¯ä¸ª chunk çš„é¢„æ ‡æ³¨æ•°é‡
"""
import json
import csv
import logging
import boto3
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# é…ç½®æ—¥å¿—
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

report_dir = Path('reports')
report_dir.mkdir(exist_ok=True)

log_file = log_dir / f'check_preannotations_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class S3Client:
    """S3/R2å®¢æˆ·ç«¯"""
    
    def __init__(self, config):
        endpoint_url = f"https://{config['account_id']}.r2.cloudflarestorage.com"
        self.s3 = boto3.client('s3',
            endpoint_url=endpoint_url,
            aws_access_key_id=config['aws_access_key_id'],
            aws_secret_access_key=config['aws_secret_access_key'],
            region_name=config['region']
        )
        self.bucket = config['bucket_name']
    
    def list_bbox_jsons(self):
        """åˆ—å‡ºæ‰€æœ‰ *_bbox.json æ–‡ä»¶"""
        bbox_files = []
        paginator = self.s3.get_paginator('list_objects_v2')
        
        for page in paginator.paginate(Bucket=self.bucket):
            for obj in page.get('Contents', []):
                if obj['Key'].endswith('_bbox.json'):
                    bbox_files.append(obj['Key'])
        
        return bbox_files
    
    def get_json(self, key):
        """ä¸‹è½½å¹¶è§£æJSONæ–‡ä»¶"""
        try:
            resp = self.s3.get_object(Bucket=self.bucket, Key=key)
            return json.loads(resp['Body'].read().decode('utf-8'))
        except Exception as e:
            logger.error(f"è·å–JSONå¤±è´¥: {key}, {e}")
            return None


def extract_chunk_id(bbox_path):
    """
    ä» bbox.json è·¯å¾„æå– chunk ID
    æ ¼å¼: 23dc/session_20260121_173554_311375/0000/down/labels/23dc_down_sbs_0000_bbox.json
    è¿”å›: session_20260121_173554_311375_0000
    """
    parts = bbox_path.split('/')
    for i, part in enumerate(parts):
        if part.startswith('session_'):
            if i + 1 < len(parts):
                chunk_num = parts[i + 1]
                return f"{part}_{chunk_num}"
    return bbox_path


def check_preannotations(config_file='config.json'):
    """æ‰«æé¢„æ ‡æ³¨ç»Ÿè®¡"""
    logger.info("="*60)
    logger.info("æŸ¥çœ‹äº‘å­˜å‚¨é¢„æ ‡æ³¨ç»Ÿè®¡")
    logger.info("="*60)
    
    # 1. åŠ è½½é…ç½®
    logger.info("\nğŸ“– åŠ è½½é…ç½®æ–‡ä»¶...")
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return
    
    s3_config = config['s3']
    s3 = S3Client(s3_config)
    
    # 2. åˆ—å‡ºæ‰€æœ‰ bbox.json
    logger.info("\nğŸ” æ‰«æé¢„æ ‡æ³¨æ–‡ä»¶...")
    print("æ­£åœ¨æ‰«æäº‘å­˜å‚¨...", end='', flush=True)
    
    bbox_files = []
    file_count = 0
    paginator = s3.s3.get_paginator('list_objects_v2')
    
    for page in paginator.paginate(Bucket=s3.bucket):
        for obj in page.get('Contents', []):
            file_count += 1
            if file_count % 1000 == 0:
                print(f"\ræ­£åœ¨æ‰«æ... {file_count} ä¸ªæ–‡ä»¶", end='', flush=True)
            if obj['Key'].endswith('_bbox.json'):
                bbox_files.append(obj['Key'])
    
    print(f"\ræ­£åœ¨æ‰«æ... {file_count} ä¸ªæ–‡ä»¶ âœ“")
    logger.info(f"âœ… æ‰«æå®Œæˆ: {file_count} ä¸ªæ–‡ä»¶, æ‰¾åˆ° {len(bbox_files)} ä¸ªé¢„æ ‡æ³¨æ–‡ä»¶")
    
    if not bbox_files:
        logger.info("æ²¡æœ‰é¢„æ ‡æ³¨æ–‡ä»¶")
        return
    
    # 3. ç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶
    summary_data = []
    details_data = {}
    
    for idx, bbox_path in enumerate(bbox_files):
        chunk_id = extract_chunk_id(bbox_path)
        print(f"\rè¯»å–é¢„æ ‡æ³¨: {idx+1}/{len(bbox_files)} - {chunk_id[:40]}", end='', flush=True)
        
        coco_data = s3.get_json(bbox_path)
        if not coco_data:
            continue
        
        images = coco_data.get('images', [])
        annotations = coco_data.get('annotations', [])
        
        # ç»Ÿè®¡æ¯å¸§çš„æ ‡æ³¨æ•°
        frame_annotations = defaultdict(int)
        for ann in annotations:
            image_id = ann['image_id']
            frame_annotations[image_id] += 1
        
        # è·å–å¸§æ–‡ä»¶åæ˜ å°„
        image_id_to_name = {img['id']: img['file_name'] for img in images}
        
        # æ±‡æ€»
        total_frames = len(images)
        annotated_frames = len(frame_annotations)
        total_annotations = len(annotations)
        
        summary_data.append({
            'chunk_id': chunk_id,
            'bbox_file': bbox_path,
            'total_frames': total_frames,
            'annotated_frames': annotated_frames,
            'total_annotations': total_annotations
        })
        
        # è¯¦æƒ…
        details_data[chunk_id] = {
            'bbox_file': bbox_path,
            'total_frames': total_frames,
            'annotated_frames': annotated_frames,
            'total_annotations': total_annotations,
            'frames': [
                {
                    'image_id': img_id,
                    'file_name': image_id_to_name.get(img_id, ''),
                    'annotation_count': count
                }
                for img_id, count in sorted(frame_annotations.items())
            ]
        }
    
    print(f"\rè¯»å–é¢„æ ‡æ³¨: {len(bbox_files)}/{len(bbox_files)} âœ“" + " " * 30)
    
    # 4. è¾“å‡ºæ±‡æ€»CSV
    summary_file = report_dir / 'preannotation_summary.csv'
    with open(summary_file, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['Chunk ID', 'é¢„æ ‡æ³¨æ–‡ä»¶', 'æ€»å¸§æ•°', 'æœ‰æ ‡æ³¨å¸§æ•°', 'æ ‡æ³¨æ€»æ•°']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in summary_data:
            writer.writerow({
                'Chunk ID': row['chunk_id'],
                'é¢„æ ‡æ³¨æ–‡ä»¶': row['bbox_file'],
                'æ€»å¸§æ•°': row['total_frames'],
                'æœ‰æ ‡æ³¨å¸§æ•°': row['annotated_frames'],
                'æ ‡æ³¨æ€»æ•°': row['total_annotations']
            })
    
    logger.info(f"\nâœ… æ±‡æ€»CSVå·²ä¿å­˜: {summary_file}")
    
    # 5. è¾“å‡ºè¯¦æƒ…JSON
    details_file = report_dir / 'preannotation_details.json'
    with open(details_file, 'w', encoding='utf-8') as f:
        json.dump(details_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… è¯¦æƒ…JSONå·²ä¿å­˜: {details_file}")
    
    # 6. æ˜¾ç¤ºæ±‡æ€»
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š é¢„æ ‡æ³¨æ±‡æ€»")
    logger.info("="*60)
    
    total_chunks = len(summary_data)
    total_frames = sum(r['total_frames'] for r in summary_data)
    total_annotated = sum(r['annotated_frames'] for r in summary_data)
    total_annotations = sum(r['total_annotations'] for r in summary_data)
    
    logger.info(f"\næ€»è®¡:")
    logger.info(f"   Chunks: {total_chunks}")
    logger.info(f"   æ€»å¸§æ•°: {total_frames}")
    logger.info(f"   æœ‰æ ‡æ³¨å¸§: {total_annotated}")
    logger.info(f"   æ ‡æ³¨æ€»æ•°: {total_annotations}")
    
    logger.info("\n" + "="*60)
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")


def main():
    check_preannotations()


if __name__ == "__main__":
    main()
