#!/usr/bin/env python3
"""
æ£€æŸ¥æ ‡æ³¨äººå‘˜æ¯æ—¥å·¥ä½œç»©æ•ˆ
- è®°å½•æ¯æ—¥å¿«ç…§
- è®¡ç®—ä»Šæ—¥äº§å‡º
- è®¡ç®—å¹³å‡é€Ÿåº¦
- è¾“å‡ºCSVæŠ¥å‘Š
"""
import requests
import json
import logging
import csv
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

# ç»©æ•ˆæŠ¥å‘Šç›®å½•
report_dir = Path('reports')
report_dir.mkdir(exist_ok=True)
snapshot_dir = report_dir / 'snapshots'
snapshot_dir.mkdir(exist_ok=True)

log_file = log_dir / f'check_performance_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CVATClient:
    """CVATå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url, api_key):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {'Authorization': f'Token {api_key}'}
    
    def get_all_tasks(self, organization_slug=None):
        """è·å–æ‰€æœ‰ä»»åŠ¡"""
        url = f'{self.base_url}/api/tasks'
        params = {'page_size': 500}
        if organization_slug:
            params['org'] = organization_slug
        
        all_tasks = []
        page = 1
        
        try:
            while True:
                params['page'] = page
                response = requests.get(url, headers=self.headers, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                results = data.get('results', [])
                all_tasks.extend(results)
                
                if not data.get('next'):
                    break
                page += 1
            
            return all_tasks
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_task_jobs(self, task_id):
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰jobs"""
        url = f'{self.base_url}/api/jobs'
        params = {'task_id': task_id, 'page_size': 1000}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            jobs_data = response.json()
            return jobs_data.get('results', [])
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–jobså¤±è´¥: task_id={task_id}, {e}")
            return []
    
    def get_job_annotations(self, job_id):
        """è·å–jobçš„æ ‡æ³¨è¯¦æƒ…"""
        url = f'{self.base_url}/api/jobs/{job_id}/annotations'
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            shapes = data.get('shapes', [])
            tracks = data.get('tracks', [])
            
            # ç»Ÿè®¡æœ‰æ ‡æ³¨çš„å¸§
            annotated_frames = set()
            for shape in shapes:
                annotated_frames.add(shape.get('frame'))
            for track in tracks:
                for shape in track.get('shapes', []):
                    annotated_frames.add(shape.get('frame'))
            
            return len(shapes), len(tracks), len(annotated_frames)
        except requests.exceptions.RequestException as e:
            logger.debug(f"è·å–æ ‡æ³¨å¤±è´¥: job_id={job_id}, {e}")
            return 0, 0, 0
    
    def get_organization_members(self, organization_slug):
        """è·å–ç»„ç»‡æˆå‘˜åˆ—è¡¨"""
        url = f'{self.base_url}/api/memberships'
        params = {'org': organization_slug, 'page_size': 100}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            return data.get('results', [])
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ç»„ç»‡æˆå‘˜å¤±è´¥: {e}")
            return []


def load_snapshot(date_str):
    """åŠ è½½æŒ‡å®šæ—¥æœŸçš„å¿«ç…§"""
    snapshot_file = snapshot_dir / f'daily_{date_str}.json'
    if snapshot_file.exists():
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def save_snapshot(date_str, data):
    """ä¿å­˜å¿«ç…§"""
    snapshot_file = snapshot_dir / f'daily_{date_str}.json'
    with open(snapshot_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    logger.info(f"âœ… å¿«ç…§å·²ä¿å­˜: {snapshot_file}")


def check_daily_performance(config_file='config.json', task_ids=None):
    """æ£€æŸ¥æ¯æ—¥ç»©æ•ˆä¸»æµç¨‹"""
    logger.info("="*60)
    logger.info("æ£€æŸ¥æ ‡æ³¨äººå‘˜æ¯æ—¥ç»©æ•ˆ")
    logger.info("="*60)
    
    today = datetime.now().strftime('%Y%m%d')
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
    
    # 1. åŠ è½½é…ç½®
    logger.info("\nğŸ“– åŠ è½½é…ç½®æ–‡ä»¶...")
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return
    
    cvat_url = config['cvat']['url']
    api_key = config['cvat']['api_key']
    organization_slug = config.get('organization', {}).get('slug')
    
    # 2. åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = CVATClient(cvat_url, api_key)
    logger.info(f"åˆå§‹åŒ–CVATå®¢æˆ·ç«¯: {cvat_url}")
    
    # 3. è·å–ä»»åŠ¡åˆ—è¡¨
    logger.info(f"\nğŸ“‹ è·å–ä»»åŠ¡åˆ—è¡¨...")
    
    if task_ids:
        tasks = []
        for task_id in task_ids:
            url = f'{cvat_url}/api/tasks/{task_id}'
            try:
                response = requests.get(url, headers=client.headers, timeout=30)
                response.raise_for_status()
                tasks.append(response.json())
            except Exception as e:
                logger.error(f"âŒ è·å–ä»»åŠ¡å¤±è´¥: task_id={task_id}, {e}")
    else:
        tasks = client.get_all_tasks(organization_slug)
    
    # æ’é™¤æ—§å¹³å°ä»»åŠ¡
    EXCLUDED_TASKS = {1967925}
    tasks = [t for t in tasks if t['id'] not in EXCLUDED_TASKS]
    
    if not tasks:
        logger.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡")
        return
    
    logger.info(f"âœ… æ‰¾åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
    
    # 4. è·å–ç»„ç»‡æˆå‘˜
    members = client.get_organization_members(organization_slug) if organization_slug else []
    user_map = {}
    for member in members:
        user = member.get('user')
        if user:
            user_map[user.get('id')] = user.get('username')
    
    # 5. æ”¶é›†æ¯ä¸ªç”¨æˆ·çš„æ•°æ®
    logger.info(f"\nğŸ“Š æ”¶é›†æ ‡æ³¨æ•°æ®...")
    
    user_data = defaultdict(lambda: {
        'total_frames': 0,
        'annotated_frames': 0,
        'total_shapes': 0,
        'total_jobs': 0,
        'completed_jobs': 0,
        'in_progress_jobs': 0,
        'jobs_detail': []
    })
    
    for task in tasks:
        task_id = task['id']
        task_name = task['name']
        
        logger.info(f"\nå¤„ç†ä»»åŠ¡: {task_name} (ID: {task_id})")
        
        jobs = client.get_task_jobs(task_id)
        if not jobs:
            continue
        
        logger.info(f"   â†’ Jobsæ•°: {len(jobs)}")
        
        # å¹¶å‘è·å–æ ‡æ³¨æ•°æ®
        def check_job(job):
            job_id = job.get('id')
            shapes, tracks, annotated_frames = client.get_job_annotations(job_id)
            return job_id, shapes, tracks, annotated_frames
        
        job_annotations = {}
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(check_job, job): job for job in jobs}
            completed = 0
            for future in as_completed(futures):
                job_id, shapes, tracks, annotated_frames = future.result()
                job_annotations[job_id] = {
                    'shapes': shapes,
                    'tracks': tracks,
                    'annotated_frames': annotated_frames
                }
                completed += 1
                if completed % 10 == 0 or completed == len(jobs):
                    print(f"\r   æ£€æŸ¥è¿›åº¦: {completed}/{len(jobs)} jobs", end='', flush=True)
        print()
        
        # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·
        for job in jobs:
            job_id = job['id']
            assignee = job.get('assignee')
            if not assignee:
                continue
            
            assignee_id = assignee.get('id')
            assignee_name = user_map.get(assignee_id, assignee.get('username', f'User_{assignee_id}'))
            
            start_frame = job.get('start_frame', 0)
            stop_frame = job.get('stop_frame', 0)
            frame_count = stop_frame - start_frame + 1
            
            ann = job_annotations.get(job_id, {'shapes': 0, 'tracks': 0, 'annotated_frames': 0})
            annotated_frames = ann['annotated_frames']
            shapes = ann['shapes']
            
            # è®¡ç®—é€Ÿåº¦ï¼ˆå¸§/å°æ—¶ï¼‰
            assigned_date = job.get('assignee_updated_date') or job.get('created_date')
            updated_date = job.get('updated_date')
            speed = None
            
            if assigned_date and updated_date and annotated_frames > 0:
                try:
                    assigned_dt = datetime.fromisoformat(assigned_date.replace('Z', '+00:00'))
                    updated_dt = datetime.fromisoformat(updated_date.replace('Z', '+00:00'))
                    hours = (updated_dt - assigned_dt).total_seconds() / 3600
                    if hours > 0:
                        speed = annotated_frames / hours
                except:
                    pass
            
            # åˆ¤æ–­çŠ¶æ€
            if annotated_frames == 0:
                status = 'not_started'
            elif annotated_frames >= frame_count:
                status = 'completed'
                user_data[assignee_name]['completed_jobs'] += 1
            else:
                status = 'in_progress'
                user_data[assignee_name]['in_progress_jobs'] += 1
            
            user_data[assignee_name]['total_frames'] += frame_count
            user_data[assignee_name]['annotated_frames'] += annotated_frames
            user_data[assignee_name]['total_shapes'] += shapes
            user_data[assignee_name]['total_jobs'] += 1
            user_data[assignee_name]['jobs_detail'].append({
                'task_id': task_id,
                'job_id': job_id,
                'frame_count': frame_count,
                'annotated_frames': annotated_frames,
                'shapes': shapes,
                'status': status,
                'speed': speed,
                'assigned_date': assigned_date,
                'updated_date': updated_date
            })
    
    # 6. åŠ è½½æ˜¨å¤©çš„å¿«ç…§è®¡ç®—ä»Šæ—¥å¢é‡
    yesterday_snapshot = load_snapshot(yesterday)
    
    # 7. è®¡ç®—ä»Šæ—¥æ•°æ®å’Œå¢é‡
    today_data = {
        'date': today,
        'generated_at': datetime.now().isoformat(),
        'users': {}
    }
    
    performance_records = []
    
    for user, data in user_data.items():
        # è®¡ç®—å¹³å‡é€Ÿåº¦
        speeds = [j['speed'] for j in data['jobs_detail'] if j['speed'] is not None]
        avg_speed = sum(speeds) / len(speeds) if speeds else None
        
        # ä»Šæ—¥å¢é‡
        today_frames = data['annotated_frames']
        today_shapes = data['total_shapes']
        
        if yesterday_snapshot and user in yesterday_snapshot.get('users', {}):
            yesterday_data = yesterday_snapshot['users'][user]
            delta_frames = today_frames - yesterday_data.get('annotated_frames', 0)
            delta_shapes = today_shapes - yesterday_data.get('total_shapes', 0)
        else:
            delta_frames = None  # æ— æ˜¨æ—¥æ•°æ®ï¼Œæ— æ³•è®¡ç®—å¢é‡
            delta_shapes = None
        
        today_data['users'][user] = {
            'total_frames': data['total_frames'],
            'annotated_frames': data['annotated_frames'],
            'total_shapes': data['total_shapes'],
            'total_jobs': data['total_jobs'],
            'completed_jobs': data['completed_jobs'],
            'in_progress_jobs': data['in_progress_jobs'],
            'avg_speed': avg_speed
        }
        
        performance_records.append({
            'date': today,
            'user': user,
            'today_frames': delta_frames if delta_frames is not None else 'N/A',
            'total_annotated_frames': data['annotated_frames'],
            'total_frames': data['total_frames'],
            'today_shapes': delta_shapes if delta_shapes is not None else 'N/A',
            'total_shapes': data['total_shapes'],
            'completed_jobs': data['completed_jobs'],
            'in_progress_jobs': data['in_progress_jobs'],
            'total_jobs': data['total_jobs'],
            'avg_speed': f"{avg_speed:.1f}" if avg_speed else 'N/A'
        })
    
    # 8. ä¿å­˜ä»Šæ—¥å¿«ç…§
    save_snapshot(today, today_data)
    
    # 9. è¾“å‡ºCSV
    csv_file = report_dir / f'daily_performance_{today}.csv'
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['date', 'user', 'today_frames', 'total_annotated_frames', 'total_frames', 
                      'today_shapes', 'total_shapes', 'completed_jobs', 'in_progress_jobs', 
                      'total_jobs', 'avg_speed']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(performance_records)
    
    logger.info(f"âœ… CSVæŠ¥å‘Šå·²ä¿å­˜: {csv_file}")
    
    # 10. è¿½åŠ åˆ°æ±‡æ€»CSV
    summary_file = report_dir / 'performance_summary.csv'
    file_exists = summary_file.exists()
    
    with open(summary_file, 'a', newline='', encoding='utf-8') as f:
        fieldnames = ['date', 'user', 'today_frames', 'total_annotated_frames', 'total_frames',
                      'today_shapes', 'total_shapes', 'completed_jobs', 'in_progress_jobs',
                      'total_jobs', 'avg_speed']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerows(performance_records)
    
    logger.info(f"âœ… æ±‡æ€»CSVå·²æ›´æ–°: {summary_file}")
    
    # 11. æ˜¾ç¤ºç»“æœ
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š ä»Šæ—¥ç»©æ•ˆæŠ¥å‘Š")
    logger.info("="*80)
    
    for record in sorted(performance_records, key=lambda x: x['total_annotated_frames'], reverse=True):
        logger.info(f"\nğŸ‘¤ {record['user']}:")
        if record['today_frames'] != 'N/A':
            logger.info(f"   ä»Šæ—¥æ ‡æ³¨: {record['today_frames']} å¸§")
        logger.info(f"   ç´¯è®¡æ ‡æ³¨: {record['total_annotated_frames']}/{record['total_frames']} å¸§")
        logger.info(f"   æ ‡æ³¨æ•°é‡: {record['total_shapes']}")
        logger.info(f"   Jobs: {record['completed_jobs']}å®Œæˆ/{record['in_progress_jobs']}è¿›è¡Œä¸­/{record['total_jobs']}æ€»è®¡")
        logger.info(f"   å¹³å‡é€Ÿåº¦: {record['avg_speed']} å¸§/å°æ—¶")
    
    logger.info("\n" + "="*80)
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import sys
    
    task_ids = None
    if len(sys.argv) > 1:
        try:
            task_ids = [int(tid) for tid in sys.argv[1:]]
            logger.info(f"æ£€æŸ¥æŒ‡å®šä»»åŠ¡: {task_ids}")
        except ValueError:
            logger.error("âŒ ä»»åŠ¡IDå¿…é¡»æ˜¯æ•°å­—")
            return
    
    check_daily_performance(task_ids=task_ids)


if __name__ == "__main__":
    main()
