#!/usr/bin/env python3
"""
å¯¼å…¥äº‘å­˜å‚¨ä¸­çš„æ–°æ•°æ®åˆ°CVAT
æŒ‰sessionåˆ†ç»„åˆ›å»ºjobsï¼Œå¹¶è‡ªåŠ¨åˆ†é…ç»™æ ‡æ³¨äººå‘˜
"""
import requests
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# é…ç½®æ—¥å¿—
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f'import_new_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CVATClient:
    """CVAT REST APIå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url, api_key):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {'Authorization': f'Token {api_key}'}
        logger.info(f"åˆå§‹åŒ–CVATå®¢æˆ·ç«¯: {base_url}")
    
    def create_task(self, name, labels, organization_slug=None):
        """åˆ›å»ºä»»åŠ¡å¹¶å®šä¹‰æ ‡ç­¾"""
        url = f'{self.base_url}/api/tasks'
        if organization_slug:
            url = f'{url}?org={organization_slug}'
        
        payload = {
            'name': name,
            'labels': labels
        }
        
        headers = {**self.headers, 'Content-Type': 'application/json'}
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            task = response.json()
            logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: ID={task['id']}, Name={name}")
            return task
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"   å“åº”å†…å®¹: {e.response.text}")
            raise
    
    def attach_data_with_jobs(self, task_id, cloud_storage_id, server_files, job_file_mapping=None):
        """ä»äº‘å­˜å‚¨åŠ è½½æ•°æ®ï¼Œå¯é€‰æ‹©æ˜¯å¦æŒ‡å®šjobåˆ†ç»„"""
        url = f'{self.base_url}/api/tasks/{task_id}/data'
        
        payload = {
            'cloud_storage_id': cloud_storage_id,
            'server_files': server_files,
            'use_cache': True,
            'image_quality': 70,
            'storage_method': 'cache',
        }
        
        if job_file_mapping:
            payload['job_file_mapping'] = job_file_mapping
            logger.info(f"   ä½¿ç”¨ job_file_mapping: {len(job_file_mapping)} ä¸ª jobs")
        else:
            payload['sorting_method'] = 'natural'
            logger.info(f"   ä¸ä½¿ç”¨ job_file_mappingï¼Œä½¿ç”¨è‡ªç„¶æ’åº")
        
        headers = {**self.headers, 'Content-Type': 'application/json'}
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            result = response.json()
            logger.info(f"âœ… æ•°æ®åŠ è½½è¯·æ±‚å·²æäº¤: task_id={task_id}")
            return result
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"   å“åº”å†…å®¹: {e.response.text}")
            raise
    
    def check_task_status(self, task_id):
        """æ£€æŸ¥ä»»åŠ¡çŠ¶æ€"""
        url = f'{self.base_url}/api/tasks/{task_id}'
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            task = response.json()
            return task.get('status'), task.get('size', 0)
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return None, 0
    
    def wait_for_data_loading(self, task_id, expected_size, timeout=3600, check_interval=30):
        """ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆ"""
        logger.info(f"â³ ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆ: task_id={task_id}, é¢„æœŸå›¾ç‰‡æ•°={expected_size}")
        
        start_time = time.time()
        last_size = 0
        
        while time.time() - start_time < timeout:
            try:
                status, current_size = self.check_task_status(task_id)
                elapsed = int(time.time() - start_time)
                
                if current_size != last_size:
                    progress_pct = (current_size * 100 // expected_size) if expected_size > 0 else 0
                    logger.info(f"   [{elapsed//60}åˆ†{elapsed%60}ç§’] è¿›åº¦: {current_size}/{expected_size} ({progress_pct}%)")
                    last_size = current_size
                
                if current_size >= expected_size * 0.95:
                    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {current_size} å¼ å›¾ç‰‡")
                    return True
                
                if status == 'failed':
                    logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: task_id={task_id}")
                    return False
                    
            except Exception as e:
                logger.warning(f"   æ£€æŸ¥è¿›åº¦æ—¶å‡ºé”™: {e}")
            
            time.sleep(check_interval)
        
        logger.warning(f"âš ï¸  æ•°æ®åŠ è½½è¶…æ—¶")
        return False
    
    def get_task_jobs(self, task_id):
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰jobs"""
        url = f'{self.base_url}/api/jobs'
        params = {'task_id': task_id, 'page_size': 1000}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            jobs_data = response.json()
            jobs = jobs_data.get('results', [])
            jobs.sort(key=lambda x: x.get('start_frame', 0))
            return jobs
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–jobså¤±è´¥: {e}")
            return []
    
    def assign_job(self, job_id, assignee_id):
        """åˆ†é…jobç»™æ ‡æ³¨äººå‘˜"""
        url = f'{self.base_url}/api/jobs/{job_id}'
        
        # æ³¨æ„ï¼šCVAT API ä½¿ç”¨ assignee å­—æ®µï¼Œä¸æ˜¯ assignee_id
        payload = {'assignee': assignee_id}
        headers = {**self.headers, 'Content-Type': 'application/json'}
        
        try:
            # PATCHæ›´æ–°
            response = requests.patch(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            
            logger.info(f"   âœ“ Job {job_id} å·²åˆ†é…ç»™ç”¨æˆ· {assignee_id}")
            return True
        except requests.exceptions.RequestException as e:
            logger.error(f"   âœ— åˆ†é…jobå¤±è´¥: job_id={job_id}, {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"   å“åº”å†…å®¹: {e.response.text}")
            return False
    
    def get_organization_members(self, organization_slug):
        """è·å–ç»„ç»‡æˆå‘˜åˆ—è¡¨"""
        url = f'{self.base_url}/api/memberships'
        params = {'org': organization_slug, 'page_size': 100}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            members = data.get('results', [])
            
            logger.info(f"âœ… è·å–ç»„ç»‡æˆå‘˜: {len(members)} äºº")
            return members
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ç»„ç»‡æˆå‘˜å¤±è´¥: {e}")
            return []


def extract_session_id(filename):
    """
    æå– chunk ID
    æ”¯æŒä¸¤ç§è·¯å¾„æ ¼å¼ï¼š
    1. æ—§æ ¼å¼: 461ff0b4__3748_session_20251210_221855_834176_0002_000000.jpg
       Chunk ID: 3748_session_20251210_221855_834176_0002
    2. æ–°æ ¼å¼: 23dc/session_20260121_200123_268461/0001/down/labels/23dc_down_sbs_0001_undetected_frames/frame_00000.jpg
       Chunk ID: session_20260121_200123_268461_0001
    """
    # å…ˆå°è¯•ä»è·¯å¾„ä¸­æå– session_xxx/xxxx æ ¼å¼
    parts = filename.split('/')
    for i, part in enumerate(parts):
        if part.startswith('session_'):
            # æ‰¾åˆ° session éƒ¨åˆ†ï¼Œå– session + ä¸‹ä¸€ä¸ªéƒ¨åˆ†ï¼ˆchunkç¼–å·ï¼‰
            if i + 1 < len(parts):
                chunk_num = parts[i + 1]
                return f"{part}_{chunk_num}"  # session_20260121_200123_268461_0001
            else:
                return part  # åªæœ‰ sessionï¼Œæ²¡æœ‰ chunk
    
    # å¦‚æœè·¯å¾„ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æ–‡ä»¶åæå–ï¼ˆæ—§æ ¼å¼ï¼‰
    basename = filename.split('/')[-1]
    if '__' in basename:
        basename = basename.split('__', 1)[1]
    
    parts = basename.split('_')
    if len(parts) >= 6 and 'session' in basename:
        return '_'.join(parts[:6])  # 3748_session_20251210_221855_834176_0002
    
    return None


def group_files_by_session(file_list):
    """æŒ‰sessionåˆ†ç»„æ–‡ä»¶"""
    sessions = defaultdict(list)
    
    for file_path in file_list:
        session_id = extract_session_id(file_path)
        if session_id:
            sessions[session_id].append(file_path)
        else:
            # æ²¡æœ‰session IDçš„æ–‡ä»¶æ”¾åˆ° 'unknown' ç»„
            sessions['unknown'].append(file_path)
    
    logger.info(f"âœ… æ–‡ä»¶åˆ†ç»„å®Œæˆ: {len(sessions)} ä¸ªsession")
    return sessions


def import_new_data(config_file='config.json', new_images_file=None):
    """å¯¼å…¥æ–°æ•°æ®ä¸»æµç¨‹"""
    logger.info("="*60)
    logger.info("å¯¼å…¥äº‘å­˜å‚¨ä¸­çš„æ–°æ•°æ®åˆ°CVAT")
    logger.info("="*60)
    
    # 1. åŠ è½½é…ç½®
    logger.info("\nğŸ“– åŠ è½½é…ç½®æ–‡ä»¶...")
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return
    
    cvat_url = config['cvat']['url']
    api_key = config['cvat']['api_key']
    cloud_storage_id = config['cloud_storage']['id']
    organization_slug = config.get('organization', {}).get('slug')
    
    # æ ‡ç­¾é…ç½®
    labels = config.get('labels', [
        {'name': 'Left hand', 'color': '#ff00ff'},
        {'name': 'Partial left hand', 'color': '#ff00ff'},
        {'name': 'Partial right hand', 'color': '#ff00ff'},
        {'name': 'Right hand', 'color': '#ff00ff'}
    ])
    
    # ä»»åŠ¡åˆ†é…é…ç½®
    assignees = config.get('assignees', [])
    use_job_mapping = config.get('use_job_file_mapping', True)
    
    # 2. è¯»å–æ–°æ•°æ®æ–‡ä»¶åˆ—è¡¨
    if not new_images_file:
        # æŸ¥æ‰¾æœ€æ–°çš„ new_images æ–‡ä»¶
        new_images_files = sorted(log_dir.glob('new_images_*.txt'), reverse=True)
        if not new_images_files:
            logger.error("âŒ æœªæ‰¾åˆ°æ–°æ•°æ®æ–‡ä»¶åˆ—è¡¨")
            logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œ check_annotation_status.py ç”Ÿæˆæ–°æ•°æ®åˆ—è¡¨")
            return
        new_images_file = new_images_files[0]
    
    logger.info(f"\nğŸ“– è¯»å–æ–°æ•°æ®æ–‡ä»¶åˆ—è¡¨: {new_images_file}")
    
    with open(new_images_file, 'r', encoding='utf-8') as f:
        new_files = [line.strip() for line in f if line.strip()]
    
    if not new_files:
        logger.info("âœ… æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¯¼å…¥")
        return
    
    logger.info(f"âœ… æ‰¾åˆ° {len(new_files)} ä¸ªæ–°æ–‡ä»¶")
    
    # 3. æŒ‰sessionåˆ†ç»„
    logger.info(f"\nğŸ“Š æŒ‰sessionåˆ†ç»„...")
    sessions = group_files_by_session(new_files)
    
    # 4. å‡†å¤‡job_file_mapping
    job_file_mapping = []
    session_names = []
    all_files = []
    
    for session_id in sorted(sessions.keys()):
        session_files = sessions[session_id]
        job_file_mapping.append(session_files)
        session_names.append(session_id)
        all_files.extend(session_files)
        logger.info(f"   Session {session_id}: {len(session_files)} å¼ å›¾ç‰‡")
    
    logger.info(f"âœ… åˆ†ç»„å®Œæˆ: {len(job_file_mapping)} ä¸ªjobs, {len(all_files)} å¼ å›¾ç‰‡")
    
    # 5. åˆ›å»ºCVATå®¢æˆ·ç«¯
    client = CVATClient(cvat_url, api_key)
    
    # 6. åˆ›å»ºä»»åŠ¡
    task_name = f"New Data Import - {datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger.info(f"\nğŸ—ï¸  åˆ›å»ºä»»åŠ¡: {task_name}")
    
    try:
        task = client.create_task(task_name, labels, organization_slug)
        task_id = task['id']
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
        return
    
    # 7. åŠ è½½å›¾ç‰‡
    logger.info(f"\nğŸ“ åŠ è½½å›¾ç‰‡...")
    logger.info(f"   æ€»å›¾ç‰‡æ•°: {len(all_files)}")
    logger.info(f"   Jobsæ•°é‡: {len(job_file_mapping)}")
    
    try:
        if use_job_mapping:
            client.attach_data_with_jobs(task_id, cloud_storage_id, all_files, job_file_mapping)
        else:
            client.attach_data_with_jobs(task_id, cloud_storage_id, all_files, None)
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return
    
    # 8. ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆ
    logger.info(f"\nâ³ ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆ...")
    if not client.wait_for_data_loading(task_id, len(all_files), timeout=3600, check_interval=30):
        logger.error(f"âŒ æ•°æ®åŠ è½½è¶…æ—¶æˆ–å¤±è´¥")
        return
    
    # 9. è·å–jobså¹¶åˆ†é…
    logger.info(f"\nğŸ‘¥ åˆ†é…ä»»åŠ¡...")
    jobs = client.get_task_jobs(task_id)
    
    if not jobs:
        logger.warning("âš ï¸  æœªæ‰¾åˆ°jobs")
    else:
        logger.info(f"   æ‰¾åˆ° {len(jobs)} ä¸ªjobs")
        
        # å¦‚æœé…ç½®äº†assigneesï¼Œè‡ªåŠ¨åˆ†é…
        if assignees:
            logger.info(f"   å¼€å§‹è‡ªåŠ¨åˆ†é…ç»™ {len(assignees)} ä¸ªæ ‡æ³¨äººå‘˜...")
            
            for idx, job in enumerate(jobs):
                job_id = job['id']
                session_id = session_names[idx] if idx < len(session_names) else 'unknown'
                
                # è½®è¯¢åˆ†é…
                assignee = assignees[idx % len(assignees)]
                assignee_id = assignee.get('id')
                assignee_name = assignee.get('name', assignee_id)
                
                if assignee_id:
                    client.assign_job(job_id, assignee_id)
                    logger.info(f"   Job {job_id} ({session_id}) â†’ {assignee_name}")
        else:
            logger.info("   â„¹ï¸  æœªé…ç½®assigneesï¼Œè·³è¿‡è‡ªåŠ¨åˆ†é…")
            logger.info("   ğŸ’¡ å¯åœ¨config.jsonä¸­é…ç½®assigneesè¿›è¡Œè‡ªåŠ¨åˆ†é…")
    
    # 10. ä¿å­˜job-sessionæ˜ å°„
    mapping_file = log_dir / f'job_session_mapping_{task_id}.json'
    mapping = []
    
    for idx, job in enumerate(jobs):
        if idx < len(session_names):
            mapping.append({
                'job_id': job['id'],
                'session_id': session_names[idx],
                'start_frame': job.get('start_frame'),
                'stop_frame': job.get('stop_frame'),
                'frame_count': job.get('stop_frame', 0) - job.get('start_frame', 0) + 1
            })
    
    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nğŸ“‹ Job-Sessionæ˜ å°„å·²ä¿å­˜: {mapping_file}")
    
    # 11. å®Œæˆ
    logger.info("\n" + "="*60)
    logger.info("âœ… å¯¼å…¥å®Œæˆï¼")
    logger.info("="*60)
    logger.info(f"ä»»åŠ¡ID: {task_id}")
    logger.info(f"ä»»åŠ¡åç§°: {task_name}")
    logger.info(f"Jobsæ•°é‡: {len(jobs)}")
    logger.info(f"æ€»å›¾ç‰‡æ•°: {len(all_files)}")
    logger.info(f"\nğŸ”— CVATé“¾æ¥: {cvat_url}/tasks/{task_id}")
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import sys
    
    new_images_file = None
    if len(sys.argv) > 1:
        new_images_file = sys.argv[1]
    
    import_new_data(new_images_file=new_images_file)


if __name__ == "__main__":
    main()
