#!/usr/bin/env python3
"""
CVATè‡ªåŠ¨åŒ–å¯¼å…¥å·¥å…· - åˆ›å»º1ä¸ªä»»åŠ¡ï¼ŒæŒ‰sessionåˆ†æˆå¤šä¸ªjobs
"""
import requests
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import re
import zipfile
import io

# é…ç½®æ—¥å¿—
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f'cvat_import_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CVATClient:
    """CVAT REST APIå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url, api_key):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {
            'Authorization': f'Token {api_key}'
        }
        logger.info(f"åˆå§‹åŒ–CVATå®¢æˆ·ç«¯: {base_url}")
    
    def create_task(self, name, labels, organization_slug=None):
        """åˆ›å»ºä»»åŠ¡å¹¶å®šä¹‰æ ‡ç­¾"""
        # åœ¨URLä¸­æŒ‡å®šç»„ç»‡
        url = f'{self.base_url}/api/tasks'
        if organization_slug:
            url = f'{url}?org={organization_slug}'
        
        payload = {
            'name': name,
            'labels': labels
        }
        
        headers = {**self.headers, 'Content-Type': 'application/json'}
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            task = response.json()
            logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: ID={task['id']}, Name={name}, Org={task.get('organization')}")
            return task
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"   å“åº”å†…å®¹: {e.response.text}")
            raise
    
    def attach_data_with_jobs(self, task_id, cloud_storage_id, server_files, job_file_mapping=None):
        """ä»äº‘å­˜å‚¨åŠ è½½æ•°æ®ï¼Œå¯é€‰æ‹©æ˜¯å¦æŒ‡å®šjobåˆ†ç»„"""
        url = f'{self.base_url}/api/tasks/{task_id}/data'
        
        payload = {
            'cloud_storage_id': cloud_storage_id,
            'server_files': server_files,
            'use_cache': True,
            'image_quality': 70,
            'storage_method': 'cache',
        }
        
        # å¦‚æœæä¾›äº† job_file_mappingï¼Œåˆ™æ·»åŠ 
        if job_file_mapping:
            payload['job_file_mapping'] = job_file_mapping
            logger.info(f"   ä½¿ç”¨ job_file_mapping: {len(job_file_mapping)} ä¸ª jobs")
        else:
            # ä¸ä½¿ç”¨ job_file_mappingï¼Œä½¿ç”¨è‡ªç„¶æ’åº
            payload['sorting_method'] = 'natural'
            logger.info(f"   ä¸ä½¿ç”¨ job_file_mappingï¼Œä½¿ç”¨è‡ªç„¶æ’åº")
        
        headers = {**self.headers, 'Content-Type': 'application/json'}
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            result = response.json()
            logger.info(f"âœ… æ•°æ®åŠ è½½è¯·æ±‚å·²æäº¤: task_id={task_id}")
            if job_file_mapping:
                logger.info(f"   å°†åˆ›å»º {len(job_file_mapping)} ä¸ªjobs")
            return result
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"   å“åº”å†…å®¹: {e.response.text}")
            raise
    
    def upload_annotations(self, task_id, annotation_data, format_name='COCO 1.0'):
        """ä¸Šä¼ æ ‡æ³¨ï¼ˆç›´æ¥ä¼ å…¥JSONæ•°æ®ï¼‰"""
        url = f'{self.base_url}/api/tasks/{task_id}/annotations'
        params = {'format': format_name}
        
        # åˆ›å»ºå†…å­˜ä¸­çš„zipæ–‡ä»¶
        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zipf:
            json_str = json.dumps(annotation_data, ensure_ascii=False, indent=2)
            zipf.writestr('annotations/instances_default.json', json_str)
        
        zip_buffer.seek(0)
        
        headers = {'Authorization': self.headers['Authorization']}
        files = {'annotation_file': ('annotations.zip', zip_buffer, 'application/zip')}
        
        try:
            response = requests.post(
                url, 
                headers=headers, 
                params=params, 
                files=files,
                timeout=120
            )
            response.raise_for_status()
            logger.info(f"âœ… æ ‡æ³¨ä¸Šä¼ æˆåŠŸ: task_id={task_id}")
            return response.status_code
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ä¸Šä¼ æ ‡æ³¨å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"   å“åº”å†…å®¹: {e.response.text}")
            raise
    
    def check_task_status(self, task_id):
        """æ£€æŸ¥ä»»åŠ¡çŠ¶æ€"""
        url = f'{self.base_url}/api/tasks/{task_id}'
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            task = response.json()
            return task.get('status')
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return None
    
    def wait_for_task_ready(self, task_id, timeout=300, check_interval=10):
        """ç­‰å¾…ä»»åŠ¡å‡†å¤‡å°±ç»ªå¹¶æ£€æŸ¥é”™è¯¯"""
        logger.info(f"â³ ç­‰å¾…ä»»åŠ¡å‡†å¤‡å°±ç»ª: task_id={task_id}")
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            status = self.check_task_status(task_id)
            if status in ['annotation', 'validation', 'completed']:
                logger.info(f"âœ… ä»»åŠ¡å·²å°±ç»ª: task_id={task_id}, status={status}")
                return True
            elif status == 'failed':
                logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: task_id={task_id}")
                return False
            
            time.sleep(check_interval)
        
        logger.warning(f"âš ï¸  ç­‰å¾…è¶…æ—¶: task_id={task_id}")
        return False
    
    def wait_for_data_loading(self, task_id, expected_size, timeout=3600, check_interval=30):
        """ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆï¼ˆæ£€æŸ¥å›¾ç‰‡æ•°é‡ï¼‰- é€‚ç”¨äºå¤§é‡å›¾ç‰‡"""
        logger.info(f"â³ ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆ: task_id={task_id}, é¢„æœŸå›¾ç‰‡æ•°={expected_size}")
        logger.info(f"   é¢„è®¡éœ€è¦æ—¶é—´: {expected_size // 100} - {expected_size // 50} åˆ†é’Ÿ")
        logger.info(f"   æ¯ {check_interval} ç§’æ£€æŸ¥ä¸€æ¬¡è¿›åº¦...")
        
        start_time = time.time()
        last_size = 0
        no_progress_count = 0
        
        while time.time() - start_time < timeout:
            try:
                url = f'{self.base_url}/api/tasks/{task_id}'
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                task = response.json()
                
                current_size = task.get('size', 0)
                status = task.get('status')
                elapsed = int(time.time() - start_time)
                
                # æ˜¾ç¤ºè¿›åº¦
                if current_size != last_size:
                    progress_pct = (current_size * 100 // expected_size) if expected_size > 0 else 0
                    remaining_time = 0
                    if current_size > 0:
                        # ä¼°ç®—å‰©ä½™æ—¶é—´
                        time_per_image = elapsed / current_size
                        remaining_images = expected_size - current_size
                        remaining_time = int(remaining_images * time_per_image / 60)
                    
                    logger.info(f"   [{elapsed//60}åˆ†{elapsed%60}ç§’] è¿›åº¦: {current_size}/{expected_size} ({progress_pct}%) | é¢„è®¡å‰©ä½™: {remaining_time}åˆ†é’Ÿ")
                    last_size = current_size
                    no_progress_count = 0
                else:
                    no_progress_count += 1
                    # å¦‚æœè¿ç»­ 5 æ¬¡æ²¡æœ‰è¿›åº¦ï¼Œæ˜¾ç¤ºç­‰å¾…ä¿¡æ¯
                    if no_progress_count % 5 == 0:
                        logger.info(f"   [{elapsed//60}åˆ†{elapsed%60}ç§’] ç­‰å¾…ä¸­... (å½“å‰: {current_size}/{expected_size})")
                
                # å¦‚æœå›¾ç‰‡æ•°è¾¾åˆ°é¢„æœŸï¼Œè®¤ä¸ºåŠ è½½å®Œæˆ
                if current_size >= expected_size * 0.95:  # å…è®¸ 5% çš„è¯¯å·®ï¼ˆå»é‡ç­‰åŸå› ï¼‰
                    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {current_size} å¼ å›¾ç‰‡ (è€—æ—¶: {elapsed//60}åˆ†{elapsed%60}ç§’)")
                    return True
                
                # å¦‚æœçŠ¶æ€å˜ä¸º failedï¼Œåœæ­¢ç­‰å¾…
                if status == 'failed':
                    logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: task_id={task_id}")
                    return False
                
                # å¦‚æœé•¿æ—¶é—´æ²¡æœ‰è¿›åº¦ï¼ˆè¶…è¿‡ 10 æ¬¡æ£€æŸ¥ï¼‰ï¼Œå¯èƒ½æœ‰é—®é¢˜
                if no_progress_count > 10 and current_size == 0:
                    logger.warning(f"âš ï¸  é•¿æ—¶é—´æ²¡æœ‰è¿›åº¦ï¼Œå¯èƒ½æ•°æ®åŠ è½½å¤±è´¥")
                    return False
                    
            except Exception as e:
                logger.warning(f"   æ£€æŸ¥è¿›åº¦æ—¶å‡ºé”™: {e}")
            
            time.sleep(check_interval)
        
        logger.warning(f"âš ï¸  æ•°æ®åŠ è½½è¶…æ—¶: å½“å‰ {last_size}/{expected_size} å›¾ç‰‡ (è¶…æ—¶æ—¶é—´: {timeout//60}åˆ†é’Ÿ)")
        return False
    
    def check_task_jobs(self, task_id):
        """æ£€æŸ¥ä»»åŠ¡çš„jobsçŠ¶æ€"""
        url = f'{self.base_url}/api/jobs'
        params = {'task_id': task_id}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            jobs = response.json()
            return jobs
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æ£€æŸ¥jobså¤±è´¥: {e}")
            return None
    
    def update_job_names(self, task_id, job_names):
        """æ›´æ–°jobåç§°"""
        # è·å–æ‰€æœ‰jobs
        url = f'{self.base_url}/api/jobs'
        params = {'task_id': task_id}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            jobs = response.json()
            
            job_list = jobs.get('results', [])
            logger.info(f"ğŸ“ æ›´æ–°jobåç§°: å…±{len(job_list)}ä¸ªjobs")
            
            # æŒ‰start_frameæ’åºï¼ˆç¡®ä¿é¡ºåºæ­£ç¡®ï¼‰
            job_list.sort(key=lambda x: x.get('start_frame', 0))
            
            # æ›´æ–°æ¯ä¸ªjobçš„åç§°
            for idx, job in enumerate(job_list):
                if idx < len(job_names):
                    job_id = job['id']
                    job_name = job_names[idx]
                    
                    # æ›´æ–°job
                    update_url = f'{self.base_url}/api/jobs/{job_id}'
                    payload = {'stage': job.get('stage'), 'state': job.get('state'), 'assignee': job.get('assignee')}
                    
                    # CVATå¯èƒ½ä¸æ”¯æŒç›´æ¥è®¾ç½®job nameï¼Œæˆ‘ä»¬å°è¯•é€šè¿‡å…¶ä»–æ–¹å¼
                    # å…ˆæ£€æŸ¥jobå¯¹è±¡æœ‰å“ªäº›å¯ç¼–è¾‘å­—æ®µ
                    logger.info(f"   Job {job_id}: {job_name} (frames: {job.get('start_frame')}-{job.get('stop_frame')})")
            
            logger.info(f"âœ… Jobåç§°è®°å½•å®Œæˆ")
            return True
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æ›´æ–°jobåç§°å¤±è´¥: {e}")
            return False
    
    def check_import_status(self, task_id, wait_time=10):
        """æ£€æŸ¥å¯¼å…¥çŠ¶æ€å’Œé”™è¯¯ - ç­‰å¾…ä¸€æ®µæ—¶é—´åæ£€æŸ¥"""
        time.sleep(wait_time)  # ç­‰å¾…å¯¼å…¥å¤„ç†
        
        url = f'{self.base_url}/api/requests'
        params = {'target': f'task/{task_id}', 'page_size': 100}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            if response.status_code == 200:
                requests_data = response.json()
                results = requests_data.get('results', [])
                
                logger.info(f"ğŸ“‹ æ£€æŸ¥å¯¼å…¥è¯·æ±‚çŠ¶æ€...")
                
                if not results:
                    logger.warning(f"   âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯¼å…¥è¯·æ±‚è®°å½•")
                    return None
                
                has_errors = False
                for req in results:
                    operation = req.get('operation')
                    status = req.get('status')
                    message = req.get('message', '')
                    
                    logger.info(f"   æ“ä½œ: {operation}, çŠ¶æ€: {status}")
                    
                    if status == 'failed':
                        has_errors = True
                        logger.error(f"   âŒ æ“ä½œå¤±è´¥!")
                        if message:
                            # æ˜¾ç¤ºå®Œæ•´é”™è¯¯ä¿¡æ¯
                            logger.error(f"   é”™è¯¯ä¿¡æ¯: {message}")
                            
                            # åˆ†æå¸¸è§é”™è¯¯
                            if 'is not specified in input files' in message:
                                logger.error(f"   âš ï¸  job_file_mapping ä¸ä¸€è‡´ - æŸäº›æ–‡ä»¶åœ¨ job_file_mapping ä¸­ä½†ä¸åœ¨ server_files ä¸­")
                                logger.error(f"   å»ºè®®: æ£€æŸ¥æ•°æ®å»é‡é€»è¾‘")
                            elif 'Could not match item id' in message:
                                logger.error(f"   âš ï¸  å›¾ç‰‡è·¯å¾„ä¸åŒ¹é… - æ ‡æ³¨æ–‡ä»¶ä¸­çš„è·¯å¾„ä¸åŠ è½½çš„å›¾ç‰‡è·¯å¾„ä¸ä¸€è‡´")
                            elif 'can\'t import annotation' in message:
                                logger.error(f"   âš ï¸  æ ‡æ³¨å¯¼å…¥å¤±è´¥ - å¯èƒ½æ˜¯labelä¸åŒ¹é…æˆ–æ ¼å¼é”™è¯¯")
                            elif 'ValidationError' in message:
                                logger.error(f"   âš ï¸  éªŒè¯é”™è¯¯ - è¯·æ±‚å‚æ•°ä¸æ­£ç¡®")
                    
                    elif status == 'finished' and message:
                        # å³ä½¿æˆåŠŸä¹Ÿå¯èƒ½æœ‰è­¦å‘Šä¿¡æ¯
                        if 'error' in message.lower() or 'warning' in message.lower():
                            logger.warning(f"   âš ï¸  æœ‰è­¦å‘Šä¿¡æ¯: {message[:200]}")
                
                return {'has_errors': has_errors, 'results': results}
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•æ£€æŸ¥å¯¼å…¥çŠ¶æ€: {e}")
        
        return None


def extract_session_id(filename):
    """æå–session ID"""
    basename = filename.split('/')[-1]
    if '__' in basename:
        basename = basename.split('__', 1)[1]
    
    parts = basename.split('_')
    if len(parts) >= 4 and 'session' in basename:
        return '_'.join(parts[:4])
    return None


def group_data_by_session(data):
    """æŒ‰sessionåˆ†ç»„æ•°æ®"""
    sessions = defaultdict(lambda: {
        'images': [],
        'annotations': [],
        'image_ids': set()
    })
    
    # åˆ†ç»„å›¾ç‰‡
    for img in data['images']:
        session_id = extract_session_id(img['file_name'])
        if session_id:
            sessions[session_id]['images'].append(img)
            sessions[session_id]['image_ids'].add(img['id'])
    
    # åˆ†ç»„æ ‡æ³¨
    for ann in data['annotations']:
        img_id = ann['image_id']
        for session_id, content in sessions.items():
            if img_id in content['image_ids']:
                content['annotations'].append(ann)
                break
    
    logger.info(f"æ•°æ®åˆ†ç»„å®Œæˆ: {len(sessions)} ä¸ªsession")
    return sessions


def create_session_annotation_data(session_data, categories):
    """ä¸ºå•ä¸ªsessionåˆ›å»ºæ ‡æ³¨æ•°æ®"""
    return {
        'images': session_data['images'],
        'annotations': session_data['annotations'],
        'categories': categories
    }


def auto_import_to_cvat(config_file='config.json'):
    """è‡ªåŠ¨åŒ–å¯¼å…¥ä¸»æµç¨‹ - åˆ›å»º1ä¸ªä»»åŠ¡ï¼ŒæŒ‰sessionåˆ†æˆjobs"""
    logger.info("="*60)
    logger.info("å¼€å§‹CVATè‡ªåŠ¨åŒ–å¯¼å…¥")
    logger.info("="*60)
    
    # 1. åŠ è½½é…ç½®
    logger.info("ğŸ“– åŠ è½½é…ç½®æ–‡ä»¶...")
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        logger.info("ğŸ’¡ è¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶ï¼Œå‚è€ƒ config.example.json")
        return
    
    cvat_url = config['cvat']['url']
    api_key = config['cvat']['api_key']
    
    # ä½¿ç”¨æ—§æ¡¶é…ç½®ï¼ˆç”¨äºä»æ—§å¹³å°è¿ç§»æ•°æ®ï¼‰
    cloud_storage_config = config.get('cloud_storage_old', config.get('cloud_storage'))
    cloud_storage_id = cloud_storage_config['id']
    logger.info(f"   ä½¿ç”¨äº‘å­˜å‚¨: {cloud_storage_config.get('name', 'Unknown')} (ID: {cloud_storage_id})")
    
    organization_slug = config.get('organization', {}).get('slug', 'wp')  # ä½¿ç”¨slugè€Œä¸æ˜¯id
    input_json = config['files']['humansignal_json']
    task_name = config.get('task', {}).get('name', 'Hand Detection - HumanSignal Import')
    
    # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ job_file_mappingï¼ˆé»˜è®¤å…³é—­ï¼‰
    use_job_mapping = config.get('use_job_file_mapping', False)
    if use_job_mapping:
        logger.info("   âš ï¸  å°†ä½¿ç”¨ job_file_mapping æŒ‰ session åˆ†ç»„")
    else:
        logger.info("   â„¹ï¸  ä¸ä½¿ç”¨ job_file_mappingï¼Œæ‰€æœ‰å›¾ç‰‡åœ¨ä¸€ä¸ªä»»åŠ¡ä¸­")
    input_json = config['files']['humansignal_json']
    task_name = config.get('task', {}).get('name', 'Hand Detection - HumanSignal Import')
    
    # 2. è¯»å–HumanSignalæ•°æ®
    logger.info(f"ğŸ“– è¯»å–HumanSignalæ•°æ®: {input_json}")
    try:
        with open(input_json, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_json}")
        return
    
    categories = data['categories']
    # æŒ‰ category ID æ’åºï¼Œç¡®ä¿ label é¡ºåºå’Œ category_id å¯¹åº”
    categories_sorted = sorted(categories, key=lambda x: x['id'])
    labels = [{'name': cat['name'], 'color': '#ff00ff'} for cat in categories_sorted]
    
    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"   æ€»å›¾ç‰‡æ•°: {len(data['images'])}")
    logger.info(f"   æ€»æ ‡æ³¨æ•°: {len(data['annotations'])}")
    logger.info(f"   ç±»åˆ«æ•°: {len(categories)}")
    logger.info(f"   ç±»åˆ«åˆ—è¡¨: {[cat['name'] for cat in categories_sorted]}")
    
    # 3. æŒ‰sessionåˆ†ç»„
    logger.info("ğŸ“Š æŒ‰sessionåˆ†ç»„æ•°æ®...")
    sessions = group_data_by_session(data)
    
    # 4. å‡†å¤‡job_file_mappingå’Œsessionåç§°
    logger.info("ğŸ—‚ï¸  å‡†å¤‡jobåˆ†ç»„æ˜ å°„...")
    job_file_mapping = []
    all_image_paths = []
    seen_paths = set()  # ç”¨äºå»é‡
    session_names = []  # è®°å½•sessionåç§°
    file_to_session = {}  # è®°å½•æ¯ä¸ªæ–‡ä»¶å±äºå“ªä¸ª sessionï¼ˆç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼‰
    
    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å”¯ä¸€çš„æ–‡ä»¶è·¯å¾„ï¼Œå¹¶è®°å½•å®ƒä»¬ç¬¬ä¸€æ¬¡å‡ºç°çš„ session
    for session_id in sorted(sessions.keys()):
        session_data = sessions[session_id]
        
        for img in session_data['images']:
            # è½¬æ¢è·¯å¾„æ ¼å¼
            path = img['file_name']  # images/461ff0b4__3748_session_xxx.jpg
            basename = path.split('/')[-1]  # 461ff0b4__3748_session_xxx.jpg
            if '__' in basename:
                basename = basename.split('__', 1)[1]  # 3748_session_xxx.jpg
            
            # äº‘å­˜å‚¨è·¯å¾„æ ¼å¼: test_1000/images/<æ–‡ä»¶å>
            final_path = f"test_1000/images/{basename}"
            
            # å»é‡ï¼šåªæ·»åŠ ä¸€æ¬¡åˆ° all_image_paths
            if final_path not in seen_paths:
                seen_paths.add(final_path)
                all_image_paths.append(final_path)
                file_to_session[final_path] = session_id  # è®°å½•ç¬¬ä¸€æ¬¡å‡ºç°çš„ session
    
    logger.info(f"   æ”¶é›†åˆ° {len(all_image_paths)} ä¸ªå”¯ä¸€æ–‡ä»¶")
    
    # ç¬¬äºŒæ­¥ï¼šæ„å»º job_file_mappingï¼ˆæ¯ä¸ªæ–‡ä»¶åªå‡ºç°ä¸€æ¬¡ï¼Œåœ¨ç¬¬ä¸€æ¬¡å‡ºç°çš„ session ä¸­ï¼‰
    for session_id in sorted(sessions.keys()):
        session_data = sessions[session_id]
        session_files = []
        seen_in_session = set()  # é˜²æ­¢åŒä¸€ä¸ª session ä¸­é‡å¤æ·»åŠ 
        
        for img in session_data['images']:
            path = img['file_name']
            basename = path.split('/')[-1]
            if '__' in basename:
                basename = basename.split('__', 1)[1]
            
            # äº‘å­˜å‚¨è·¯å¾„æ ¼å¼: test_1000/images/<æ–‡ä»¶å>
            final_path = f"test_1000/images/{basename}"
            
            # åªæ·»åŠ å±äºå½“å‰ session çš„æ–‡ä»¶ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°åœ¨è¿™ä¸ª sessionï¼‰
            # å¹¶ä¸”åœ¨å½“å‰ session ä¸­è¿˜æ²¡æœ‰æ·»åŠ è¿‡
            if file_to_session.get(final_path) == session_id and final_path not in seen_in_session:
                session_files.append(final_path)
                seen_in_session.add(final_path)
        
        if session_files:  # åªæ·»åŠ éç©ºçš„session
            job_file_mapping.append(session_files)
            session_names.append(session_id)
            logger.info(f"   Session {session_id}: {len(session_files)} å¼ å›¾ç‰‡")
    
    logger.info(f"âœ… åˆ†ç»„å®Œæˆ: {len(job_file_mapping)} ä¸ªjobs, {len(all_image_paths)} å¼ å›¾ç‰‡ï¼ˆå»é‡åï¼‰")
    
    # éªŒè¯ï¼šç¡®ä¿ job_file_mapping ä¸­çš„æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ all_image_paths ä¸­
    logger.info("ğŸ” éªŒè¯ job_file_mapping ä¸€è‡´æ€§...")
    all_files_in_mapping = set()
    for session_files in job_file_mapping:
        all_files_in_mapping.update(session_files)
    
    missing_files = all_files_in_mapping - set(all_image_paths)
    if missing_files:
        logger.error(f"âŒ å‘ç°ä¸ä¸€è‡´: {len(missing_files)} ä¸ªæ–‡ä»¶åœ¨ job_file_mapping ä¸­ä½†ä¸åœ¨ server_files ä¸­")
        for f in list(missing_files)[:10]:
            logger.error(f"   - {f}")
        logger.error(f"   è¿™ä¼šå¯¼è‡´ CVAT æ‹’ç»è¯·æ±‚ï¼Œè¯·æ£€æŸ¥æ•°æ®")
        return
    
    extra_files = set(all_image_paths) - all_files_in_mapping
    if extra_files:
        logger.warning(f"âš ï¸  {len(extra_files)} ä¸ªæ–‡ä»¶åœ¨ server_files ä¸­ä½†ä¸åœ¨ä»»ä½• job ä¸­")
    
    logger.info(f"âœ… éªŒè¯é€šè¿‡: job_file_mapping å’Œ server_files ä¸€è‡´")
    logger.info(f"   - server_files æ–‡ä»¶æ•°: {len(all_image_paths)}")
    logger.info(f"   - job_file_mapping æ–‡ä»¶æ•°: {len(all_files_in_mapping)}")
    logger.info(f"   - æ€»å¼•ç”¨æ¬¡æ•°: {sum(len(files) for files in job_file_mapping)}")
    
    # 5. åˆ›å»ºCVATå®¢æˆ·ç«¯
    client = CVATClient(cvat_url, api_key)
    
    # 6. åˆ›å»ºä»»åŠ¡
    logger.info(f"\nğŸ—ï¸  åˆ›å»ºä»»åŠ¡: {task_name}")
    try:
        task = client.create_task(task_name, labels, organization_slug)
        task_id = task['id']
        logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: ID={task_id}")
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
        return
    
    # 7. åŠ è½½å›¾ç‰‡å¹¶æŒ‡å®šjobåˆ†ç»„
    logger.info(f"\nğŸ“ åŠ è½½å›¾ç‰‡å¹¶åˆ›å»ºjobs...")
    logger.info(f"   æ€»å›¾ç‰‡æ•°: {len(all_image_paths)}")
    logger.info(f"   Jobsæ•°é‡: {len(job_file_mapping)}")
    
    # ä¿å­˜è°ƒè¯•ä¿¡æ¯
    debug_file = log_dir / f'debug_request_{task_id}.json'
    with open(debug_file, 'w', encoding='utf-8') as f:
        json.dump({
            'task_id': task_id,
            'server_files_count': len(all_image_paths),
            'server_files_sample': all_image_paths[:10],
            'job_file_mapping_count': len(job_file_mapping),
            'job_file_mapping_sample': [files[:5] for files in job_file_mapping[:3]],
            'session_names': session_names[:10]
        }, f, indent=2, ensure_ascii=False)
    logger.info(f"   è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜: {debug_file}")
    
    try:
        if use_job_mapping:
            # ä½¿ç”¨ job_file_mapping
            client.attach_data_with_jobs(
                task_id, 
                cloud_storage_id, 
                all_image_paths,
                job_file_mapping
            )
        else:
            # ä¸ä½¿ç”¨ job_file_mapping
            client.attach_data_with_jobs(
                task_id, 
                cloud_storage_id, 
                all_image_paths,
                None  # ä¸ä¼  job_file_mapping
            )
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        logger.error(f"   è¯·æ£€æŸ¥è°ƒè¯•æ–‡ä»¶: {debug_file}")
        return
    
    # 8. ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆï¼ˆæ£€æŸ¥å›¾ç‰‡æ•°é‡ï¼‰
    logger.info(f"\nâ³ ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆ...")
    logger.info(f"   æç¤º: 21,000+ å¼ å›¾ç‰‡é¢„è®¡éœ€è¦ 15-30 åˆ†é’Ÿ")
    logger.info(f"   è¯·è€å¿ƒç­‰å¾…ï¼Œè„šæœ¬ä¼šæ¯ 30 ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦")
    
    if not client.wait_for_data_loading(task_id, len(all_image_paths), timeout=3600, check_interval=30):  # 60åˆ†é’Ÿè¶…æ—¶ï¼Œæ¯30ç§’æ£€æŸ¥
        logger.error(f"âŒ æ•°æ®åŠ è½½è¶…æ—¶æˆ–å¤±è´¥")
        logger.info(f"   å»ºè®®: æ‰‹åŠ¨æ£€æŸ¥ CVAT ä»»åŠ¡çŠ¶æ€: {cvat_url}/tasks/{task_id}")
        # æ£€æŸ¥å¯¼å…¥çŠ¶æ€
        client.check_import_status(task_id)
        return
    
    # 8.5 æ£€æŸ¥jobsåˆ›å»ºæƒ…å†µå¹¶æ›´æ–°åç§°
    logger.info(f"\nğŸ” æ£€æŸ¥jobsåˆ›å»ºæƒ…å†µ...")
    jobs_data = client.check_task_jobs(task_id)
    if jobs_data:
        job_count = jobs_data.get('count', 0)
        logger.info(f"   å®é™…åˆ›å»ºçš„jobsæ•°é‡: {job_count}")
        if job_count != len(job_file_mapping):
            logger.warning(f"   âš ï¸  é¢„æœŸ{len(job_file_mapping)}ä¸ªjobsï¼Œå®é™…{job_count}ä¸ª")
        
        # å°è¯•æ›´æ–°jobåç§°
        if job_count == len(session_names):
            logger.info(f"\nğŸ“ ä¸ºjobsè®¾ç½®sessionåç§°...")
            client.update_job_names(task_id, session_names)
    
    # 8.6 æ£€æŸ¥æ•°æ®åŠ è½½çŠ¶æ€
    logger.info(f"\nğŸ” æ£€æŸ¥æ•°æ®åŠ è½½çŠ¶æ€...")
    load_status = client.check_import_status(task_id, wait_time=10)
    if load_status and load_status.get('has_errors'):
        logger.error(f"\nâŒ æ•°æ®åŠ è½½æœ‰é”™è¯¯ï¼Œè¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")
        logger.error(f"   å»ºè®®: æ£€æŸ¥äº‘å­˜å‚¨ä¸­çš„å›¾ç‰‡è·¯å¾„æ˜¯å¦æ­£ç¡®")
    
    # 9. ä¸Šä¼ æ ‡æ³¨
    logger.info(f"\nğŸ“¤ ä¸Šä¼ æ ‡æ³¨...")
    
    # è½¬æ¢æ ‡æ³¨æ–‡ä»¶ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼Œä½¿å…¶ä¸åŠ è½½çš„å›¾ç‰‡è·¯å¾„ä¸€è‡´
    # é‡è¦ï¼šåªåŒ…å«å®é™…åŠ è½½çš„å›¾ç‰‡ï¼
    logger.info(f"   è½¬æ¢æ ‡æ³¨æ–‡ä»¶ä¸­çš„å›¾ç‰‡è·¯å¾„...")
    
    # åˆ›å»ºå®é™…åŠ è½½çš„æ–‡ä»¶åé›†åˆï¼ˆç”¨äºè¿‡æ»¤ï¼‰
    loaded_files_set = set(all_image_paths)
    logger.info(f"   å®é™…åŠ è½½çš„æ–‡ä»¶æ•°: {len(loaded_files_set)}")
    
    converted_data = data.copy()
    converted_images = []
    path_mapping = {}  # åŸå§‹è·¯å¾„ -> æ–°è·¯å¾„
    loaded_image_ids = set()  # å®é™…åŠ è½½çš„å›¾ç‰‡ ID
    
    for img in data['images']:
        original_path = img['file_name']
        
        # è½¬æ¢è·¯å¾„
        basename = original_path.split('/')[-1]
        if '__' in basename:
            basename = basename.split('__', 1)[1]
        
        # æ·»åŠ  prefix å‰ç¼€
        new_path = f"test_1000/images/{basename}"
        
        # åªåŒ…å«å®é™…åŠ è½½çš„æ–‡ä»¶ï¼ˆç”¨å®Œæ•´è·¯å¾„åŒ¹é…ï¼‰
        if new_path not in loaded_files_set:
            continue
        
        # è®°å½•æ˜ å°„å…³ç³»
        path_mapping[original_path] = new_path
        loaded_image_ids.add(img['id'])
        
        # åˆ›å»ºæ–°çš„imageå¯¹è±¡
        new_img = img.copy()
        new_img['file_name'] = new_path
        converted_images.append(new_img)
    
    # ç¡®ä¿ categories æ ¼å¼æ­£ç¡®ï¼ˆCVAT éœ€è¦ category_id ä» 1 å¼€å§‹ï¼‰
    logger.info(f"ğŸ·ï¸  å¤„ç†ç±»åˆ«ä¿¡æ¯...")
    converted_categories = []
    category_id_mapping = {}  # æ—§ID -> æ–°ID
    
    for idx, cat in enumerate(categories_sorted):
        new_id = idx + 1  # ä» 1 å¼€å§‹ï¼Œè€Œä¸æ˜¯ä» 0
        category_id_mapping[cat['id']] = new_id
        converted_cat = {
            'id': new_id,
            'name': cat['name'],
            'supercategory': cat.get('supercategory', '')
        }
        converted_categories.append(converted_cat)
    
    logger.info(f"   ç±»åˆ«IDæ˜ å°„: {category_id_mapping}")
    
    # åªåŒ…å«å·²åŠ è½½å›¾ç‰‡çš„æ ‡æ³¨ï¼Œå¹¶è½¬æ¢ category_id
    converted_annotations = []
    for ann in data['annotations']:
        if ann['image_id'] in loaded_image_ids:
            new_ann = ann.copy()
            old_cat_id = ann['category_id']
            new_ann['category_id'] = category_id_mapping.get(old_cat_id, old_cat_id + 1)
            converted_annotations.append(new_ann)
    
    converted_data['images'] = converted_images
    converted_data['annotations'] = converted_annotations
    converted_data['categories'] = converted_categories
    
    logger.info(f"   å·²è½¬æ¢ {len(converted_images)} ä¸ªå›¾ç‰‡è·¯å¾„")
    logger.info(f"   åŒ…å« {len(converted_annotations)} ä¸ªæ ‡æ³¨")
    logger.info(f"   ç±»åˆ«æ•°: {len(converted_categories)}")
    logger.info(f"   ç±»åˆ«IDæ˜ å°„: {category_id_mapping}")
    if converted_images:
        logger.info(f"   ç¤ºä¾‹è·¯å¾„: {converted_images[0]['file_name']}")
    else:
        logger.error(f"   âŒ æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å›¾ç‰‡ï¼è¯·æ£€æŸ¥è·¯å¾„è½¬æ¢é€»è¾‘")
    
    # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ ‡æ³¨å’Œç±»åˆ«ä¿¡æ¯
    if converted_annotations:
        logger.info(f"   ç¤ºä¾‹æ ‡æ³¨: image_id={converted_annotations[0]['image_id']}, category_id={converted_annotations[0]['category_id']}")
    if converted_categories:
        logger.info(f"   ç±»åˆ«åˆ—è¡¨: {[cat['name'] for cat in converted_categories]}")
    
    try:
        client.upload_annotations(task_id, converted_data)
        logger.info(f"âœ… æ ‡æ³¨ä¸Šä¼ è¯·æ±‚å·²æäº¤")
        
        # ç­‰å¾…æ ‡æ³¨å¤„ç†å®Œæˆ - 39,000+ ä¸ªæ ‡æ³¨éœ€è¦æ—¶é—´
        logger.info(f"\nâ³ ç­‰å¾…æ ‡æ³¨å¯¼å…¥å®Œæˆ...")
        logger.info(f"   æç¤º: 39,000+ ä¸ªæ ‡æ³¨é¢„è®¡éœ€è¦ 5-10 åˆ†é’Ÿ")
        logger.info(f"   æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # ç­‰å¾…æ›´é•¿æ—¶é—´
        for i in range(20):  # æœ€å¤šç­‰å¾… 10 åˆ†é’Ÿï¼ˆæ¯æ¬¡ 30 ç§’ï¼‰
            time.sleep(30)
            annotation_status = client.check_import_status(task_id, wait_time=0)
            
            if annotation_status:
                results = annotation_status.get('results', [])
                for req in results:
                    if 'import:annotations' in req.get('operation', ''):
                        status = req.get('status')
                        progress = req.get('progress', 0)
                        
                        if status == 'finished':
                            logger.info(f"âœ… æ ‡æ³¨å¯¼å…¥å®Œæˆ")
                            break
                        elif status == 'failed':
                            logger.error(f"âŒ æ ‡æ³¨å¯¼å…¥å¤±è´¥")
                            if req.get('message'):
                                logger.error(f"   é”™è¯¯: {req.get('message')[:500]}")
                            return
                        else:
                            logger.info(f"   è¿›åº¦: {progress}% - çŠ¶æ€: {status}")
                else:
                    continue
                break
        
        # æœ€ç»ˆæ£€æŸ¥
        logger.info(f"\nğŸ” æœ€ç»ˆæ£€æŸ¥æ ‡æ³¨å¯¼å…¥çŠ¶æ€...")
        annotation_status = client.check_import_status(task_id, wait_time=5)
        
        if annotation_status and annotation_status.get('has_errors'):
            logger.error(f"\nâŒ æ ‡æ³¨å¯¼å…¥æœ‰é”™è¯¯ï¼")
            logger.error(f"   å¸¸è§åŸå› :")
            logger.error(f"   1. æ ‡æ³¨æ–‡ä»¶ä¸­çš„å›¾ç‰‡è·¯å¾„ä¸å®é™…åŠ è½½çš„å›¾ç‰‡è·¯å¾„ä¸åŒ¹é…")
            logger.error(f"   2. æ ‡æ³¨çš„category_idä¸ä»»åŠ¡çš„labelä¸åŒ¹é…")
            logger.error(f"   3. æ ‡æ³¨æ ¼å¼ä¸æ­£ç¡®")
            return
        else:
            logger.info(f"âœ… æ ‡æ³¨å¯¼å…¥æ£€æŸ¥å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ ä¸Šä¼ æ ‡æ³¨å¤±è´¥: {e}")
        return
    
    # 10. å®Œæˆ - ä¿å­˜jobå’Œsessionçš„æ˜ å°„å…³ç³»
    logger.info("\n" + "="*60)
    logger.info("âœ… å¯¼å…¥å®Œæˆï¼")
    logger.info("="*60)
    logger.info(f"ä»»åŠ¡ID: {task_id}")
    logger.info(f"ä»»åŠ¡åç§°: {task_name}")
    logger.info(f"Jobsæ•°é‡: {len(job_file_mapping)}")
    logger.info(f"æ€»å›¾ç‰‡æ•°: {len(all_image_paths)}")
    logger.info(f"æ€»æ ‡æ³¨æ•°: {len(data['annotations'])}")
    logger.info(f"\nğŸ”— CVATé“¾æ¥: {cvat_url}/tasks/{task_id}")
    logger.info(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    # ä¿å­˜jobå’Œsessionçš„æ˜ å°„å…³ç³»
    mapping_file = log_dir / f'job_session_mapping_{task_id}.json'
    
    # è·å–å®é™…çš„jobåˆ—è¡¨
    jobs_response = client.check_task_jobs(task_id)
    if jobs_response:
        job_list = jobs_response.get('results', [])
        job_list.sort(key=lambda x: x.get('start_frame', 0))
        
        mapping = []
        for idx, job in enumerate(job_list):
            if idx < len(session_names):
                mapping.append({
                    'job_id': job['id'],
                    'session_id': session_names[idx],
                    'start_frame': job.get('start_frame'),
                    'stop_frame': job.get('stop_frame'),
                    'frame_count': job.get('stop_frame', 0) - job.get('start_frame', 0) + 1
                })
        
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ“‹ Job-Sessionæ˜ å°„å·²ä¿å­˜: {mapping_file}")
        logger.info(f"\nå‰5ä¸ªæ˜ å°„:")
        for item in mapping[:5]:
            logger.info(f"   Job {item['job_id']}: {item['session_id']} ({item['frame_count']} å¸§)")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    auto_import_to_cvat()


if __name__ == "__main__":
    main()
