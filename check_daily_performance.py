#!/usr/bin/env python3
"""
æ£€æŸ¥æ ‡æ³¨äººå‘˜æ¯æ—¥å·¥ä½œç»©æ•ˆ
- åŸºäº job çº§åˆ«çš„å¿«ç…§å·®å€¼ç»Ÿè®¡
- å¢é‡å½’å±åˆ°å½“å‰ assigneeï¼ˆè§£å†³é‡æ–°åˆ†é…é—®é¢˜ï¼‰
- æ”¯æŒæŸ¥è¯¢æŒ‡å®šæ—¥æœŸ
- è¾“å‡ºCSVæŠ¥å‘Š
"""
import requests
import json
import logging
import csv
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

# ç»©æ•ˆæŠ¥å‘Šç›®å½•
report_dir = Path('reports')
report_dir.mkdir(exist_ok=True)
snapshot_dir = report_dir / 'snapshots'
snapshot_dir.mkdir(exist_ok=True)

log_file = log_dir / f'check_performance_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ’é™¤çš„ä»»åŠ¡
EXCLUDED_TASKS = {1967925}


class CVATClient:
    """CVATå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url, api_key):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {'Authorization': f'Token {api_key}'}
    
    def get_all_tasks(self, organization_slug=None):
        """è·å–æ‰€æœ‰ä»»åŠ¡"""
        url = f'{self.base_url}/api/tasks'
        params = {'page_size': 500}
        if organization_slug:
            params['org'] = organization_slug
        
        all_tasks = []
        page = 1
        
        try:
            while True:
                params['page'] = page
                response = requests.get(url, headers=self.headers, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                all_tasks.extend(data.get('results', []))
                if not data.get('next'):
                    break
                page += 1
            return all_tasks
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_task_jobs(self, task_id):
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰jobs"""
        url = f'{self.base_url}/api/jobs'
        params = {'task_id': task_id, 'page_size': 1000}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            return response.json().get('results', [])
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–jobså¤±è´¥: task_id={task_id}, {e}")
            return []
    
    def get_job_annotated_frames(self, job_id):
        """è·å–jobçš„å·²æ ‡æ³¨å¸§æ•°"""
        url = f'{self.base_url}/api/jobs/{job_id}/annotations'
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            annotated_frames = set()
            for shape in data.get('shapes', []):
                annotated_frames.add(shape.get('frame'))
            for track in data.get('tracks', []):
                for shape in track.get('shapes', []):
                    annotated_frames.add(shape.get('frame'))
            
            return len(annotated_frames), len(data.get('shapes', []))
        except:
            return 0, 0
    
    def get_organization_members(self, organization_slug):
        """è·å–ç»„ç»‡æˆå‘˜åˆ—è¡¨"""
        url = f'{self.base_url}/api/memberships'
        params = {'org': organization_slug, 'page_size': 100}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            return response.json().get('results', [])
        except:
            return []


def load_snapshot(date_str):
    """åŠ è½½æŒ‡å®šæ—¥æœŸçš„å¿«ç…§"""
    snapshot_file = snapshot_dir / f'daily_{date_str}.json'
    if snapshot_file.exists():
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def save_snapshot(date_str, data):
    """ä¿å­˜å¿«ç…§ï¼ˆåŒä¸€å¤©è¦†ç›–ï¼‰"""
    snapshot_file = snapshot_dir / f'daily_{date_str}.json'
    with open(snapshot_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    logger.info(f"âœ… å¿«ç…§å·²ä¿å­˜: {snapshot_file}")



def check_daily_performance(config_file='config.json', target_date=None):
    """
    æ£€æŸ¥æ¯æ—¥ç»©æ•ˆä¸»æµç¨‹
    target_date: æŸ¥è¯¢çš„æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œé»˜è®¤ä»Šå¤©
    """
    logger.info("="*60)
    logger.info("æ£€æŸ¥æ ‡æ³¨äººå‘˜æ¯æ—¥ç»©æ•ˆ")
    logger.info("="*60)
    
    # ç¡®å®šæ—¥æœŸ
    today = datetime.now().strftime('%Y%m%d')
    if target_date:
        query_date = target_date
    else:
        query_date = today
    
    query_date_display = f"{query_date[:4]}-{query_date[4:6]}-{query_date[6:8]}"
    logger.info(f"ğŸ“… æŸ¥è¯¢æ—¥æœŸ: {query_date_display}")
    
    # 1. åŠ è½½é…ç½®
    logger.info("\nğŸ“– åŠ è½½é…ç½®æ–‡ä»¶...")
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return
    
    cvat_url = config['cvat']['url']
    api_key = config['cvat']['api_key']
    organization_slug = config.get('organization', {}).get('slug')
    
    client = CVATClient(cvat_url, api_key)
    logger.info(f"åˆå§‹åŒ–CVATå®¢æˆ·ç«¯: {cvat_url}")
    
    # 2. è·å–ä»»åŠ¡åˆ—è¡¨
    logger.info(f"\nğŸ“‹ è·å–ä»»åŠ¡åˆ—è¡¨...")
    tasks = client.get_all_tasks(organization_slug)
    tasks = [t for t in tasks if t['id'] not in EXCLUDED_TASKS]
    
    if not tasks:
        logger.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡")
        return
    
    logger.info(f"âœ… æ‰¾åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
    
    # 3. è·å–ç»„ç»‡æˆå‘˜
    members = client.get_organization_members(organization_slug) if organization_slug else []
    user_map = {}
    for member in members:
        user = member.get('user')
        if user:
            user_map[user.get('id')] = user.get('username')
    
    # 4. æ”¶é›†å½“å‰æ‰€æœ‰ job çš„æ•°æ®
    logger.info(f"\nğŸ“Š æ”¶é›†æ ‡æ³¨æ•°æ®...")
    
    # job_data: {job_id: {assignee, annotated_frames, shapes, frame_count, ...}}
    job_data = {}
    
    for task in tasks:
        task_id = task['id']
        task_name = task['name']
        
        logger.info(f"\nå¤„ç†ä»»åŠ¡: {task_name} (ID: {task_id})")
        
        jobs = client.get_task_jobs(task_id)
        if not jobs:
            continue
        
        logger.info(f"   â†’ Jobsæ•°: {len(jobs)}")
        
        # å¹¶å‘è·å–æ ‡æ³¨æ•°æ®
        def check_job(job):
            job_id = job.get('id')
            annotated_frames, shapes = client.get_job_annotated_frames(job_id)
            return job, annotated_frames, shapes
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(check_job, job) for job in jobs]
            completed = 0
            
            for future in as_completed(futures):
                job, annotated_frames, shapes = future.result()
                job_id = job['id']
                assignee = job.get('assignee')
                
                start_frame = job.get('start_frame', 0)
                stop_frame = job.get('stop_frame', 0)
                frame_count = stop_frame - start_frame + 1
                
                assignee_id = assignee.get('id') if assignee else None
                assignee_name = user_map.get(assignee_id, assignee.get('username')) if assignee else None
                
                job_data[job_id] = {
                    'task_id': task_id,
                    'assignee_id': assignee_id,
                    'assignee_name': assignee_name,
                    'frame_count': frame_count,
                    'annotated_frames': annotated_frames,
                    'shapes': shapes,
                    'updated_date': job.get('updated_date'),
                    'assigned_date': job.get('assignee_updated_date') or job.get('created_date')
                }
                
                completed += 1
                if completed % 10 == 0 or completed == len(jobs):
                    print(f"\r   æ£€æŸ¥è¿›åº¦: {completed}/{len(jobs)} jobs", end='', flush=True)
            
            print()
    
    # 5. åŠ è½½æ˜¨å¤©çš„å¿«ç…§
    yesterday = (datetime.strptime(query_date, '%Y%m%d') - timedelta(days=1)).strftime('%Y%m%d')
    yesterday_snapshot = load_snapshot(yesterday)
    yesterday_jobs = yesterday_snapshot.get('jobs', {}) if yesterday_snapshot else {}
    
    # 6. è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æ•°æ®
    user_stats = defaultdict(lambda: {
        'today_frames': 0,      # ä»Šæ—¥å¢é‡å¸§æ•°
        'today_shapes': 0,      # ä»Šæ—¥å¢é‡æ ‡æ³¨æ•°
        'total_frames': 0,      # ç´¯è®¡æ ‡æ³¨å¸§æ•°
        'total_shapes': 0,      # ç´¯è®¡æ ‡æ³¨æ•°
        'job_frames': 0,        # åˆ†é…çš„æ€»å¸§æ•°
        'jobs': 0,
        'speeds': []
    })
    
    for job_id, data in job_data.items():
        assignee_name = data['assignee_name']
        if not assignee_name:
            continue
        
        # ç´¯è®¡æ•°æ®
        user_stats[assignee_name]['total_frames'] += data['annotated_frames']
        user_stats[assignee_name]['total_shapes'] += data['shapes']
        user_stats[assignee_name]['job_frames'] += data['frame_count']
        user_stats[assignee_name]['jobs'] += 1
        
        # è®¡ç®—å¢é‡ï¼ˆä¸æ˜¨å¤©å¿«ç…§å¯¹æ¯”ï¼‰
        job_id_str = str(job_id)
        if job_id_str in yesterday_jobs:
            yesterday_frames = yesterday_jobs[job_id_str].get('annotated_frames', 0)
            yesterday_shapes = yesterday_jobs[job_id_str].get('shapes', 0)
            delta_frames = data['annotated_frames'] - yesterday_frames
            delta_shapes = data['shapes'] - yesterday_shapes
            
            # å¢é‡å½’å±åˆ°å½“å‰ assigneeï¼ˆå³ä½¿ job è¢«é‡æ–°åˆ†é…äº†ï¼‰
            if delta_frames > 0:
                user_stats[assignee_name]['today_frames'] += delta_frames
            if delta_shapes > 0:
                user_stats[assignee_name]['today_shapes'] += delta_shapes
        else:
            # æ–° jobï¼Œå…¨éƒ¨ç®—ä»Šæ—¥å¢é‡
            user_stats[assignee_name]['today_frames'] += data['annotated_frames']
            user_stats[assignee_name]['today_shapes'] += data['shapes']
        
        # è®¡ç®—é€Ÿåº¦
        if data['annotated_frames'] > 0 and data['assigned_date'] and data['updated_date']:
            try:
                assigned_dt = datetime.fromisoformat(data['assigned_date'].replace('Z', '+00:00'))
                updated_dt = datetime.fromisoformat(data['updated_date'].replace('Z', '+00:00'))
                hours = (updated_dt - assigned_dt).total_seconds() / 3600
                if hours > 0.1:
                    speed = data['annotated_frames'] / hours
                    user_stats[assignee_name]['speeds'].append(speed)
            except:
                pass
    
    # 7. ä¿å­˜ä»Šæ—¥å¿«ç…§ï¼ˆåªåœ¨æŸ¥è¯¢ä»Šå¤©æ—¶ä¿å­˜ï¼‰
    if query_date == today:
        snapshot_data = {
            'date': today,
            'generated_at': datetime.now().isoformat(),
            'jobs': {str(k): {'annotated_frames': v['annotated_frames'], 'shapes': v['shapes']} 
                     for k, v in job_data.items()}
        }
        save_snapshot(today, snapshot_data)
    
    # 8. ç”ŸæˆæŠ¥å‘Š
    performance_records = []
    
    for user, stats in user_stats.items():
        avg_speed = sum(stats['speeds']) / len(stats['speeds']) if stats['speeds'] else None
        
        performance_records.append({
            'date': query_date,
            'user': user,
            'today_frames': stats['today_frames'],
            'today_shapes': stats['today_shapes'],
            'total_frames': stats['total_frames'],
            'total_shapes': stats['total_shapes'],
            'job_frames': stats['job_frames'],
            'jobs': stats['jobs'],
            'avg_speed': f"{avg_speed:.1f}" if avg_speed else 'N/A'
        })
    
    # æŒ‰ä»Šæ—¥å¸§æ•°æ’åº
    performance_records.sort(key=lambda x: x['today_frames'], reverse=True)
    
    # 9. è¾“å‡ºCSV
    csv_file = report_dir / f'daily_performance_{query_date}.csv'
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['æ—¥æœŸ', 'ç”¨æˆ·', 'å½“æ—¥å¸§æ•°', 'å½“æ—¥æ ‡æ³¨æ•°', 
                      'ç´¯è®¡å¸§æ•°', 'ç´¯è®¡æ ‡æ³¨æ•°', 'åˆ†é…å¸§æ•°', 'Jobsæ•°', 'å¹³å‡é€Ÿåº¦']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for record in performance_records:
            writer.writerow({
                'æ—¥æœŸ': record['date'],
                'ç”¨æˆ·': record['user'],
                'å½“æ—¥å¸§æ•°': record['today_frames'],
                'å½“æ—¥æ ‡æ³¨æ•°': record['today_shapes'],
                'ç´¯è®¡å¸§æ•°': record['total_frames'],
                'ç´¯è®¡æ ‡æ³¨æ•°': record['total_shapes'],
                'åˆ†é…å¸§æ•°': record['job_frames'],
                'Jobsæ•°': record['jobs'],
                'å¹³å‡é€Ÿåº¦': record['avg_speed']
            })
    
    logger.info(f"\nâœ… CSVæŠ¥å‘Šå·²ä¿å­˜: {csv_file}")
    
    # 10. è¿½åŠ åˆ°æ±‡æ€»CSV
    summary_file = report_dir / 'performance_summary.csv'
    file_exists = summary_file.exists()
    
    with open(summary_file, 'a', newline='', encoding='utf-8') as f:
        fieldnames = ['æ—¥æœŸ', 'ç”¨æˆ·', 'å½“æ—¥å¸§æ•°', 'å½“æ—¥æ ‡æ³¨æ•°',
                      'ç´¯è®¡å¸§æ•°', 'ç´¯è®¡æ ‡æ³¨æ•°', 'åˆ†é…å¸§æ•°', 'Jobsæ•°', 'å¹³å‡é€Ÿåº¦']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        for record in performance_records:
            writer.writerow({
                'æ—¥æœŸ': record['date'],
                'ç”¨æˆ·': record['user'],
                'å½“æ—¥å¸§æ•°': record['today_frames'],
                'å½“æ—¥æ ‡æ³¨æ•°': record['today_shapes'],
                'ç´¯è®¡å¸§æ•°': record['total_frames'],
                'ç´¯è®¡æ ‡æ³¨æ•°': record['total_shapes'],
                'åˆ†é…å¸§æ•°': record['job_frames'],
                'Jobsæ•°': record['jobs'],
                'å¹³å‡é€Ÿåº¦': record['avg_speed']
            })
    
    logger.info(f"âœ… æ±‡æ€»CSVå·²æ›´æ–°: {summary_file}")
    
    # 11. æ˜¾ç¤ºç»“æœ
    logger.info("\n" + "="*80)
    logger.info(f"ğŸ“Š {query_date_display} ç»©æ•ˆæŠ¥å‘Š")
    logger.info("="*80)
    
    total_today = sum(r['today_frames'] for r in performance_records)
    has_yesterday = yesterday_snapshot is not None
    
    if not has_yesterday:
        logger.info(f"\nâš ï¸  æ—  {yesterday} å¿«ç…§ï¼Œä»Šæ—¥å¢é‡ä¸ºå…¨éƒ¨ç´¯è®¡å€¼")
    
    logger.info(f"\nğŸ“ˆ å½“æ—¥æ€»äº§å‡º: {total_today} å¸§")
    
    for record in performance_records:
        logger.info(f"\nğŸ‘¤ {record['user']}:")
        logger.info(f"   ä»Šæ—¥æ ‡æ³¨: {record['today_frames']} å¸§ ({record['today_shapes']} ä¸ªæ ‡æ³¨)")
        logger.info(f"   ç´¯è®¡æ ‡æ³¨: {record['total_frames']}/{record['job_frames']} å¸§")
        logger.info(f"   Jobsæ•°: {record['jobs']}")
        logger.info(f"   å¹³å‡é€Ÿåº¦: {record['avg_speed']} å¸§/å°æ—¶")
    
    logger.info("\n" + "="*80)
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")


def main():
    import sys
    
    target_date = None
    create_snapshot = False
    
    # è§£æå‚æ•°
    args = sys.argv[1:]
    for arg in args:
        if arg == '--snapshot':
            create_snapshot = True
        elif len(arg) == 8 and arg.isdigit():
            target_date = arg
        else:
            print("ç”¨æ³•: python check_daily_performance.py [YYYYMMDD] [--snapshot]")
            print("ç¤ºä¾‹:")
            print("  python check_daily_performance.py              # æŸ¥è¯¢ä»Šå¤©")
            print("  python check_daily_performance.py 20260129     # æŸ¥è¯¢æŒ‡å®šæ—¥æœŸ")
            print("  python check_daily_performance.py 20260128 --snapshot  # è¡¥å½•æŒ‡å®šæ—¥æœŸå¿«ç…§")
            return
    
    if create_snapshot:
        if not target_date:
            print("âŒ è¡¥å½•å¿«ç…§éœ€è¦æŒ‡å®šæ—¥æœŸ")
            print("ç¤ºä¾‹: python check_daily_performance.py 20260128 --snapshot")
            return
        create_snapshot_for_date(target_date)
    else:
        check_daily_performance(target_date=target_date)


def create_snapshot_for_date(target_date):
    """è¡¥å½•æŒ‡å®šæ—¥æœŸçš„å¿«ç…§ï¼ˆåŸºäº job çš„ updated_date æ™ºèƒ½åˆ¤æ–­ï¼‰"""
    logger.info("="*60)
    logger.info(f"è¡¥å½•å¿«ç…§: {target_date}")
    logger.info("="*60)
    
    # ç›®æ ‡æ—¥æœŸçš„ç»“æŸæ—¶é—´ç‚¹ï¼ˆå½“å¤©23:59:59ï¼‰
    target_date_end = f"{target_date[:4]}-{target_date[4:6]}-{target_date[6:8]}T23:59:59"
    logger.info(f"ğŸ“… åªåŒ…å« {target_date_end} ä¹‹å‰æ›´æ–°çš„æ•°æ®")
    
    # åŠ è½½é…ç½®
    try:
        with open('config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    cvat_url = config['cvat']['url']
    api_key = config['cvat']['api_key']
    organization_slug = config.get('organization', {}).get('slug')
    
    client = CVATClient(cvat_url, api_key)
    
    # è·å–ä»»åŠ¡
    logger.info("\nğŸ“‹ è·å–ä»»åŠ¡åˆ—è¡¨...")
    tasks = client.get_all_tasks(organization_slug)
    tasks = [t for t in tasks if t['id'] not in EXCLUDED_TASKS]
    logger.info(f"âœ… æ‰¾åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
    
    # æ”¶é›† job æ•°æ®
    logger.info("\nğŸ“Š æ”¶é›†æ ‡æ³¨æ•°æ®...")
    job_data = {}
    included_count = 0
    excluded_count = 0
    
    for task in tasks:
        task_id = task['id']
        jobs = client.get_task_jobs(task_id)
        
        for job in jobs:
            job_id = job['id']
            updated_date = job.get('updated_date', '')
            
            # åˆ¤æ–­ job çš„ updated_date æ˜¯å¦åœ¨ç›®æ ‡æ—¥æœŸä¹‹å‰
            if updated_date and updated_date <= target_date_end:
                # è¿™ä¸ª job åœ¨ç›®æ ‡æ—¥æœŸä¹‹å‰æœ‰æ›´æ–°ï¼Œè·å–å½“å‰æ•°æ®
                annotated_frames, shapes = client.get_job_annotated_frames(job_id)
                job_data[str(job_id)] = {
                    'annotated_frames': annotated_frames,
                    'shapes': shapes
                }
                included_count += 1
            else:
                # è¿™ä¸ª job åœ¨ç›®æ ‡æ—¥æœŸä¹‹åæ‰æœ‰æ›´æ–°ï¼Œå­˜ä¸º0
                job_data[str(job_id)] = {
                    'annotated_frames': 0,
                    'shapes': 0
                }
                excluded_count += 1
        
        print(f"\r   å·²å¤„ç†: {included_count + excluded_count} jobs (åŒ…å«: {included_count}, æ’é™¤: {excluded_count})", end='', flush=True)
    
    print()
    
    # ä¿å­˜å¿«ç…§
    snapshot_data = {
        'date': target_date,
        'generated_at': datetime.now().isoformat(),
        'note': f'è¡¥å½•å¿«ç…§ï¼ŒåŸºäº updated_date <= {target_date_end}',
        'jobs': job_data
    }
    save_snapshot(target_date, snapshot_data)
    
    logger.info(f"\nâœ… å¿«ç…§è¡¥å½•å®Œæˆ: {target_date}")
    logger.info(f"   åŒ…å«æ•°æ®çš„ jobs: {included_count}")
    logger.info(f"   æ’é™¤çš„ jobsï¼ˆä¹‹åæ›´æ–°ï¼‰: {excluded_count}")


if __name__ == "__main__":
    main()
