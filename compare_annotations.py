#!/usr/bin/env python3
"""
å¯¹æ¯”é¢„æ ‡æ³¨å’Œäººå·¥æ ‡æ³¨çŠ¶æ€
æ‰¾å‡ºå“ªäº›jobæœ‰é¢„æ ‡æ³¨ä½†è¿˜æ²¡äººå·¥æ ‡æ³¨
"""
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import json
import logging
import csv
from pathlib import Path
from datetime import datetime

# é…ç½®å¸¦é‡è¯•çš„ session
def get_retry_session():
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

# é…ç½®æ—¥å¿—
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f'compare_annotations_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ’é™¤çš„ä»»åŠ¡
EXCLUDED_TASKS = {1967925}

# æŠ¥å‘Šç›®å½•
report_dir = Path('reports')
report_dir.mkdir(exist_ok=True)


class CVATClient:
    def __init__(self, base_url, api_key, org=None):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.org = org
        self.headers = {'Authorization': f'Token {api_key}'}
        self.session = get_retry_session()
    
    def get_tasks(self):
        """è·å–æ‰€æœ‰ä»»åŠ¡"""
        url = f'{self.base_url}/api/tasks'
        params = {'page_size': 500}
        if self.org:
            params['org'] = self.org
        
        tasks = []
        page = 1
        while True:
            params['page'] = page
            response = self.session.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            results = data.get('results', [])
            tasks.extend(results)
            if not data.get('next'):
                break
            page += 1
        return tasks
    
    def get_task_jobs(self, task_id):
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰jobs"""
        url = f'{self.base_url}/api/jobs'
        params = {'task_id': task_id, 'page_size': 1000}
        response = self.session.get(url, headers=self.headers, params=params, timeout=30)
        response.raise_for_status()
        return response.json().get('results', [])
    
    def get_job_frames(self, job_id):
        """è·å–jobçš„å¸§ä¿¡æ¯"""
        url = f'{self.base_url}/api/jobs/{job_id}/data/meta'
        response = self.session.get(url, headers=self.headers, timeout=60)
        response.raise_for_status()
        return response.json()
    
    def get_job_annotations(self, job_id):
        """è·å–jobçš„æ ‡æ³¨è¯¦æƒ…"""
        url = f'{self.base_url}/api/jobs/{job_id}/annotations'
        response = self.session.get(url, headers=self.headers, timeout=30)
        response.raise_for_status()
        return response.json()


def extract_chunk_id(filename):
    """ä»æ–‡ä»¶è·¯å¾„æå–chunk ID"""
    parts = filename.split('/')
    for i, part in enumerate(parts):
        if part.startswith('session_'):
            if i + 1 < len(parts):
                chunk_num = parts[i + 1]
                return f"{part}_{chunk_num}"
    return None


def load_preannotation_details():
    """åŠ è½½é¢„æ ‡æ³¨è¯¦æƒ…"""
    details_file = report_dir / 'preannotation_details.json'
    if not details_file.exists():
        return None
    with open(details_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def compare_annotations(config_file='config.json'):
    """å¯¹æ¯”é¢„æ ‡æ³¨å’Œäººå·¥æ ‡æ³¨"""
    logger.info("="*60)
    logger.info("å¯¹æ¯”é¢„æ ‡æ³¨å’Œäººå·¥æ ‡æ³¨çŠ¶æ€")
    logger.info("="*60)
    
    # 1. åŠ è½½é…ç½®
    logger.info("\nğŸ“– åŠ è½½é…ç½®...")
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return
    
    cvat_url = config['cvat']['url']
    api_key = config['cvat']['api_key']
    org = config['cvat'].get('org')
    
    cvat = CVATClient(cvat_url, api_key, org)
    
    # 2. åŠ è½½é¢„æ ‡æ³¨è¯¦æƒ…
    logger.info("ğŸ“– åŠ è½½é¢„æ ‡æ³¨æ•°æ®...")
    preannotation_data = load_preannotation_details()
    if not preannotation_data:
        logger.error("âŒ é¢„æ ‡æ³¨æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œé€‰é¡¹10ç”Ÿæˆ")
        return
    
    logger.info(f"   é¢„æ ‡æ³¨chunks: {len(preannotation_data)}")
    
    # 3. è·å–æ‰€æœ‰ä»»åŠ¡
    logger.info("\nğŸ“‹ è·å–CVATä»»åŠ¡...")
    try:
        tasks = cvat.get_tasks()
        tasks = [t for t in tasks if t['id'] not in EXCLUDED_TASKS]
        logger.info(f"   ä»»åŠ¡æ•°: {len(tasks)}")
    except Exception as e:
        logger.error(f"âŒ è·å–ä»»åŠ¡å¤±è´¥: {e}")
        return
    
    # 4. éå†æ‰€æœ‰jobsï¼Œå¯¹æ¯”çŠ¶æ€
    results = []
    
    for task in tasks:
        task_id = task['id']
        task_name = task['name']
        logger.info(f"\nå¤„ç†ä»»åŠ¡: {task_name} (ID: {task_id})")
        
        try:
            jobs = cvat.get_task_jobs(task_id)
        except Exception as e:
            logger.error(f"   âŒ è·å–jobså¤±è´¥: {e}")
            continue
        
        for job in jobs:
            job_id = job['id']
            assignee = job.get('assignee', {})
            assignee_name = assignee.get('username', 'æœªåˆ†é…') if assignee else 'æœªåˆ†é…'
            start_frame = job.get('start_frame', 0)
            stop_frame = job.get('stop_frame', 0)
            frame_count = stop_frame - start_frame + 1
            
            # è·å–äººå·¥æ ‡æ³¨æ•°
            human_annotated = 0
            try:
                ann_data = cvat.get_job_annotations(job_id)
                human_frames = set()
                for shape in ann_data.get('shapes', []):
                    human_frames.add(shape.get('frame'))
                human_annotated = len(human_frames)
            except:
                pass
            
            # è·å–jobçš„å¸§è·¯å¾„ï¼Œç¡®å®šchunk_id
            chunk_id = None
            try:
                meta = cvat.get_job_frames(job_id)
                frames = meta.get('frames', [])
                if frames:
                    first_frame = frames[0].get('name', '')
                    chunk_id = extract_chunk_id(first_frame)
            except Exception as e:
                logger.debug(f"   è·å–å¸§ä¿¡æ¯å¤±è´¥: {e}")
            
            # æŸ¥æ‰¾é¢„æ ‡æ³¨æ•°æ®
            pre_frames = 0
            pre_annotations = 0
            if chunk_id and chunk_id in preannotation_data:
                pre_info = preannotation_data[chunk_id]
                pre_frames = pre_info.get('annotated_frames', 0)
                pre_annotations = pre_info.get('total_annotations', 0)
            
            results.append({
                'task_id': task_id,
                'task_name': task_name,
                'job_id': job_id,
                'assignee': assignee_name,
                'total_frames': frame_count,
                'human_annotated': human_annotated,
                'pre_frames': pre_frames,
                'pre_annotations': pre_annotations,
                'chunk_id': chunk_id or ''
            })
            
            # æ—¥å¿—è¾“å‡ºå…³é”®ä¿¡æ¯
            if pre_frames > 0 and human_annotated == 0:
                logger.info(f"   Job {job_id}: æœ‰{pre_frames}å¸§é¢„æ ‡æ³¨ï¼Œæ— äººå·¥æ ‡æ³¨ âš ï¸")
    
    # 5. è¾“å‡ºCSVæŠ¥å‘Š
    csv_file = report_dir / f'annotation_comparison_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['ä»»åŠ¡ID', 'ä»»åŠ¡åç§°', 'Job ID', 'è´Ÿè´£äºº', 'æ€»å¸§æ•°', 
                        'äººå·¥æ ‡æ³¨å¸§', 'é¢„æ ‡æ³¨å¸§', 'é¢„æ ‡æ³¨æ•°', 'Chunk ID', 'çŠ¶æ€'])
        
        for r in results:
            # åˆ¤æ–­çŠ¶æ€
            if r['human_annotated'] > 0:
                status = 'å·²æ ‡æ³¨'
            elif r['pre_frames'] > 0:
                status = 'å¾…æ ‡æ³¨(æœ‰é¢„æ ‡æ³¨)'
            else:
                status = 'å¾…æ ‡æ³¨(æ— é¢„æ ‡æ³¨)'
            
            writer.writerow([
                r['task_id'], r['task_name'], r['job_id'], r['assignee'],
                r['total_frames'], r['human_annotated'], r['pre_frames'],
                r['pre_annotations'], r['chunk_id'], status
            ])
    
    logger.info(f"\nâœ… CSVæŠ¥å‘Š: {csv_file}")
    
    # 6. ç»Ÿè®¡æ±‡æ€»
    total_jobs = len(results)
    human_done = sum(1 for r in results if r['human_annotated'] > 0)
    with_pre = sum(1 for r in results if r['pre_frames'] > 0 and r['human_annotated'] == 0)
    no_pre = sum(1 for r in results if r['pre_frames'] == 0 and r['human_annotated'] == 0)
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
    logger.info("="*60)
    logger.info(f"   æ€»Jobsæ•°: {total_jobs}")
    logger.info(f"   å·²æœ‰äººå·¥æ ‡æ³¨: {human_done}")
    logger.info(f"   æœ‰é¢„æ ‡æ³¨å¾…äººå·¥: {with_pre}")
    logger.info(f"   æ— é¢„æ ‡æ³¨å¾…äººå·¥: {no_pre}")
    logger.info("="*60)
    
    # 7. è¾“å‡ºJSONè¯¦æƒ…
    json_file = report_dir / 'annotation_comparison.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    logger.info(f"ğŸ“‹ JSONè¯¦æƒ…: {json_file}")
    
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")


def main():
    compare_annotations()


if __name__ == "__main__":
    main()
