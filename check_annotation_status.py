#!/usr/bin/env python3
"""
æ ¸å¯¹äº‘å­˜å‚¨å’ŒCVATå¹³å°çš„æ ‡æ³¨çŠ¶æ€
æ‰¾å‡ºå“ªäº›æ•°æ®å·²æ ‡æ³¨ã€å“ªäº›æœªæ ‡æ³¨
"""
import requests
import json
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import boto3
    from botocore.exceptions import ClientError, NoCredentialsError
    HAS_BOTO3 = True
except ImportError:
    HAS_BOTO3 = False

# é…ç½®æ—¥å¿—
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f'check_status_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CVATClient:
    """CVATå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url, api_key):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {'Authorization': f'Token {api_key}'}
        logger.info(f"åˆå§‹åŒ–CVATå®¢æˆ·ç«¯: {base_url}")
    
    def get_all_tasks(self, organization_slug=None):
        """è·å–æ‰€æœ‰ä»»åŠ¡"""
        url = f'{self.base_url}/api/tasks'
        params = {'page_size': 500}
        if organization_slug:
            params['org'] = organization_slug
        
        all_tasks = []
        page = 1
        
        try:
            while True:
                params['page'] = page
                response = requests.get(url, headers=self.headers, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                results = data.get('results', [])
                all_tasks.extend(results)
                
                if not data.get('next'):
                    break
                page += 1
            
            logger.info(f"âœ… è·å–ä»»åŠ¡åˆ—è¡¨æˆåŠŸ: {len(all_tasks)} ä¸ªä»»åŠ¡")
            return all_tasks
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_task_data(self, task_id):
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰å›¾ç‰‡"""
        url = f'{self.base_url}/api/tasks/{task_id}/data/meta'
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            frames = data.get('frames', [])
            images = [frame.get('name') for frame in frames if frame.get('name')]
            
            logger.info(f"   ä»»åŠ¡ {task_id}: {len(images)} å¼ å›¾ç‰‡")
            return images
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡æ•°æ®å¤±è´¥: task_id={task_id}, {e}")
            return []
    
    def get_task_jobs(self, task_id):
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰jobs"""
        url = f'{self.base_url}/api/jobs'
        params = {'task_id': task_id, 'page_size': 1000}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            jobs_data = response.json()
            jobs = jobs_data.get('results', [])
            return jobs
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–jobså¤±è´¥: task_id={task_id}, {e}")
            return []
    
    def get_job_has_annotations(self, job_id):
        """æ£€æŸ¥jobæ˜¯å¦æœ‰æ ‡æ³¨ï¼ˆåªæ£€æŸ¥æ•°é‡ï¼Œä¸è·å–å…¨éƒ¨æ•°æ®ï¼‰"""
        url = f'{self.base_url}/api/jobs/{job_id}/annotations'
        
        try:
            # åªè·å–ç¬¬ä¸€é¡µï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            response = requests.get(url, headers=self.headers, params={'page_size': 1}, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰shapesæˆ–tracks
            has_shapes = len(data.get('shapes', [])) > 0
            has_tracks = len(data.get('tracks', [])) > 0
            
            return has_shapes or has_tracks
        except requests.exceptions.Timeout:
            logger.debug(f"æ£€æŸ¥job {job_id}è¶…æ—¶")
            return False
        except requests.exceptions.RequestException as e:
            logger.debug(f"æ£€æŸ¥job {job_id}å¤±è´¥: {e}")
            return False


def list_s3_files(bucket_name, prefix, aws_access_key_id=None, aws_secret_access_key=None, region_name='us-east-1', account_id=None):
    """åˆ—ä¸¾S3/R2å­˜å‚¨æ¡¶ä¸­çš„æ–‡ä»¶
    
    Args:
        bucket_name: S3 bucketåç§°
        prefix: æ–‡ä»¶è·¯å¾„å‰ç¼€
        aws_access_key_id: AWS Access Key ID
        aws_secret_access_key: AWS Secret Access Key
        region_name: AWS Region
        account_id: Cloudflare R2 Account IDï¼ˆå¦‚æœä½¿ç”¨R2ï¼‰
        
    Returns:
        æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    if not HAS_BOTO3:
        logger.error("âŒ boto3æœªå®‰è£…ï¼Œæ— æ³•è®¿é—®S3")
        logger.info("ğŸ’¡ å®‰è£…: pip install boto3")
        return None
    
    try:
        # åˆ¤æ–­æ˜¯å¦æ˜¯Cloudflare R2
        if account_id:
            # Cloudflare R2 endpoint
            endpoint_url = f'https://{account_id}.r2.cloudflarestorage.com'
            logger.info(f"   ä½¿ç”¨Cloudflare R2: {endpoint_url}")
            s3_client = boto3.client(
                's3',
                endpoint_url=endpoint_url,
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                region_name='auto'
            )
        else:
            # æ ‡å‡†AWS S3
            if aws_access_key_id and aws_secret_access_key:
                s3_client = boto3.client(
                    's3',
                    aws_access_key_id=aws_access_key_id,
                    aws_secret_access_key=aws_secret_access_key,
                    region_name=region_name
                )
            else:
                # ä½¿ç”¨é»˜è®¤å‡­è¯
                s3_client = boto3.client('s3', region_name=region_name)
        
        logger.info(f"   æ­£åœ¨åˆ—ä¸¾æ–‡ä»¶: {bucket_name}/{prefix}")
        
        # å…ˆåˆ—ä¸¾æ ¹ç›®å½•çœ‹çœ‹æœ‰ä»€ä¹ˆ
        logger.info(f"   å…ˆæ£€æŸ¥æ ¹ç›®å½•...")
        try:
            root_response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix='', MaxKeys=10)
            if 'Contents' in root_response:
                logger.info(f"   æ ¹ç›®å½•ç¤ºä¾‹æ–‡ä»¶:")
                for obj in root_response['Contents'][:5]:
                    logger.info(f"     - {obj['Key']}")
        except Exception as e:
            logger.warning(f"   æ— æ³•åˆ—ä¸¾æ ¹ç›®å½•: {e}")
        
        # åˆ—ä¸¾å¯¹è±¡
        files = []
        paginator = s3_client.get_paginator('list_objects_v2')
        
        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
            if 'Contents' in page:
                for obj in page['Contents']:
                    key = obj['Key']
                    # åªè¦æ–‡ä»¶ï¼Œä¸è¦ç›®å½•
                    if not key.endswith('/'):
                        files.append(key)
        
        logger.info(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
        return files
        
    except NoCredentialsError:
        logger.error("âŒ AWSå‡­è¯æœªæ‰¾åˆ°")
        logger.info("ğŸ’¡ åœ¨config.jsonä¸­é…ç½®s3éƒ¨åˆ†")
        return None
    except ClientError as e:
        logger.error(f"âŒ S3è®¿é—®å¤±è´¥: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ åˆ—ä¸¾æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def extract_basename(file_path):
    """æå–æ–‡ä»¶åŸºç¡€åï¼ˆå»æ‰è·¯å¾„å’Œhashå‰ç¼€ï¼‰"""
    basename = file_path.split('/')[-1]
    if '__' in basename:
        basename = basename.split('__', 1)[1]
    return basename


def check_annotation_status(config_file='config.json', task_ids=None):
    """æ ¸å¯¹æ ‡æ³¨çŠ¶æ€ä¸»æµç¨‹
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        task_ids: å¯é€‰çš„ä»»åŠ¡IDåˆ—è¡¨ï¼Œå¦‚æœæŒ‡å®šåˆ™åªæ£€æŸ¥è¿™äº›ä»»åŠ¡
    """
    logger.info("="*60)
    logger.info("æ ¸å¯¹CVATå¹³å°çš„æ ‡æ³¨çŠ¶æ€")
    logger.info("="*60)
    
    # 1. åŠ è½½é…ç½®
    logger.info("\nğŸ“– åŠ è½½é…ç½®æ–‡ä»¶...")
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return
    
    cvat_url = config['cvat']['url']
    api_key = config['cvat']['api_key']
    organization_slug = config.get('organization', {}).get('slug')
    
    # S3é…ç½®
    s3_config = config.get('s3', {})
    bucket_name = s3_config.get('bucket_name')
    prefix = s3_config.get('prefix', 'test_1000/images/')
    aws_access_key_id = s3_config.get('aws_access_key_id')
    aws_secret_access_key = s3_config.get('aws_secret_access_key')
    region_name = s3_config.get('region', 'us-east-1')
    account_id = s3_config.get('account_id')  # Cloudflare R2 Account ID
    
    # 2. åˆå§‹åŒ–CVATå®¢æˆ·ç«¯
    cvat_client = CVATClient(cvat_url, api_key)
    
    # 3. ä»S3/R2è·å–äº‘å­˜å‚¨æ–‡ä»¶åˆ—è¡¨
    cloud_basenames = None
    
    if bucket_name:
        logger.info(f"\nğŸ“ ä»äº‘å­˜å‚¨è·å–æ–‡ä»¶åˆ—è¡¨...")
        logger.info(f"   Bucket: {bucket_name}")
        logger.info(f"   Prefix: {prefix}")
        
        s3_files = list_s3_files(
            bucket_name=bucket_name,
            prefix=prefix,
            aws_access_key_id=aws_access_key_id,
            aws_secret_access_key=aws_secret_access_key,
            region_name=region_name,
            account_id=account_id
        )
        
        if s3_files:
            # æŒ‰ session åˆ†ç»„æ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ json æ–‡ä»¶ï¼ˆæ ‡å¿— session å®Œæ•´ï¼‰
            logger.info(f"   æ£€æŸ¥ session å®Œæ•´æ€§ï¼ˆæ˜¯å¦æœ‰ json æ–‡ä»¶ï¼‰...")
            
            session_files = defaultdict(lambda: {'images': [], 'has_json': False})
            
            for file_path in s3_files:
                # æå– session ID
                # è·¯å¾„æ ¼å¼: b1e0/session_20260108_034622_359267/0000/down/labels/xxx/frame_00089.jpg
                parts = file_path.split('/')
                session_id = None
                for part in parts:
                    if part.startswith('session_'):
                        session_id = part
                        break
                
                if not session_id:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ json æ–‡ä»¶
                if file_path.endswith('.json'):
                    session_files[session_id]['has_json'] = True
                elif file_path.endswith('.jpg') or file_path.endswith('.png'):
                    session_files[session_id]['images'].append(file_path)
            
            # åªä¿ç•™æœ‰ json æ–‡ä»¶çš„å®Œæ•´ session
            complete_sessions = {sid: data for sid, data in session_files.items() if data['has_json']}
            incomplete_sessions = {sid: data for sid, data in session_files.items() if not data['has_json']}
            
            logger.info(f"   å®Œæ•´ session: {len(complete_sessions)} ä¸ª")
            logger.info(f"   ä¸å®Œæ•´ sessionï¼ˆæ— jsonï¼‰: {len(incomplete_sessions)} ä¸ª")
            
            if incomplete_sessions:
                logger.info(f"   ä¸å®Œæ•´çš„ session å°†è¢«è·³è¿‡:")
                for sid in sorted(incomplete_sessions.keys())[:5]:
                    img_count = len(incomplete_sessions[sid]['images'])
                    logger.info(f"      - {sid}: {img_count} å¼ å›¾ç‰‡ï¼ˆæ— jsonæ–‡ä»¶ï¼‰")
                if len(incomplete_sessions) > 5:
                    logger.info(f"      ... è¿˜æœ‰ {len(incomplete_sessions) - 5} ä¸ª")
            
            # æ„å»º cloud_basenames å’Œ cloud_path_mapï¼ˆåªåŒ…å«å®Œæ•´ session çš„å›¾ç‰‡ï¼‰
            cloud_basenames = set()
            cloud_path_map = {}  # basename -> å®Œæ•´è·¯å¾„çš„æ˜ å°„
            
            for session_id, data in complete_sessions.items():
                for file_path in data['images']:
                    basename = file_path.split('/')[-1]
                    # å»æ‰hashå‰ç¼€
                    if '__' in basename:
                        basename = basename.split('__', 1)[1]
                    cloud_basenames.add(basename)
                    # è®°å½•ç¬¬ä¸€æ¬¡å‡ºç°çš„å®Œæ•´è·¯å¾„
                    if basename not in cloud_path_map:
                        cloud_path_map[basename] = file_path
            
            logger.info(f"âœ… äº‘å­˜å‚¨æ–‡ä»¶ï¼ˆå®Œæ•´sessionï¼‰: {len(cloud_basenames)} ä¸ª")
        else:
            logger.warning("âš ï¸  æ— æ³•ä»S3è·å–æ–‡ä»¶åˆ—è¡¨")
            cloud_path_map = {}
    else:
        logger.warning("âš ï¸  æœªé…ç½®S3ï¼Œå°†åªç»Ÿè®¡CVATä¸­çš„æ•°æ®")
        logger.info("ğŸ’¡ åœ¨config.jsonä¸­æ·»åŠ s3é…ç½®ï¼š")
        cloud_path_map = {}
    
    # 4. è·å–CVATä¸­çš„æ‰€æœ‰ä»»åŠ¡å’Œå›¾ç‰‡
    logger.info(f"\nğŸ“‹ è·å–CVATä»»åŠ¡åˆ—è¡¨...")
    
    if task_ids:
        # ä½¿ç”¨æŒ‡å®šçš„ä»»åŠ¡ID
        tasks = []
        for task_id in task_ids:
            url = f'{cvat_url}/api/tasks/{task_id}'
            try:
                response = requests.get(url, headers={'Authorization': f'Token {api_key}'}, timeout=30)
                response.raise_for_status()
                tasks.append(response.json())
            except Exception as e:
                logger.error(f"âŒ è·å–ä»»åŠ¡å¤±è´¥: task_id={task_id}, {e}")
    else:
        # è·å–æ‰€æœ‰ä»»åŠ¡
        tasks = cvat_client.get_all_tasks(organization_slug)
    
    if not tasks:
        logger.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡")
        cvat_images = set()
        cvat_annotated_images = set()
    else:
        logger.info(f"âœ… æ‰¾åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
        
        cvat_images = set()
        cvat_annotated_images = set()
        
        logger.info(f"\nğŸ“Š åˆ†æä»»åŠ¡æ•°æ®...")
        for idx, task in enumerate(tasks, 1):
            task_id = task['id']
            task_name = task['name']
            
            logger.info(f"\n[{idx}/{len(tasks)}] å¤„ç†ä»»åŠ¡: {task_name} (ID: {task_id})")
            
            # è·å–ä»»åŠ¡å›¾ç‰‡åˆ—è¡¨
            images = cvat_client.get_task_data(task_id)
            image_basenames = []
            for img_path in images:
                basename = extract_basename(img_path)
                cvat_images.add(basename)
                image_basenames.append(basename)
            
            logger.info(f"   â†’ å›¾ç‰‡æ•°: {len(images)}")
            
            # è·å–ä»»åŠ¡çš„jobs
            logger.info(f"   â†’ è·å–jobs...")
            jobs = cvat_client.get_task_jobs(task_id)
            logger.info(f"   â†’ Jobsæ•°: {len(jobs)}")
            
            # æ£€æŸ¥æ¯ä¸ªjobæ˜¯å¦æœ‰æ ‡æ³¨ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
            logger.info(f"   â†’ æ£€æŸ¥æ ‡æ³¨çŠ¶æ€ï¼ˆå¹¶å‘æ£€æŸ¥ï¼‰...")
            annotated_job_count = 0
            
            # å‡†å¤‡æ£€æŸ¥ä»»åŠ¡
            def check_job(job_info):
                job_idx, job = job_info
                job_id = job.get('id')
                start_frame = job.get('start_frame', 0)
                stop_frame = job.get('stop_frame', 0)
                frame_count = stop_frame - start_frame + 1
                
                has_annotations = cvat_client.get_job_has_annotations(job_id)
                
                return {
                    'job_idx': job_idx,
                    'job_id': job_id,
                    'start_frame': start_frame,
                    'stop_frame': stop_frame,
                    'frame_count': frame_count,
                    'has_annotations': has_annotations
                }
            
            # å¹¶å‘æ£€æŸ¥ï¼ˆæœ€å¤š10ä¸ªå¹¶å‘ï¼‰
            results = []
            with ThreadPoolExecutor(max_workers=10) as executor:
                futures = {executor.submit(check_job, (idx, job)): idx for idx, job in enumerate(jobs, 1)}
                
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if len(results) % 10 == 0 or len(results) == len(jobs):
                        logger.info(f"      è¿›åº¦: {len(results)}/{len(jobs)} jobs")
            
            # æŒ‰jobé¡ºåºæ’åºç»“æœ
            results.sort(key=lambda x: x['job_idx'])
            
            # å¤„ç†ç»“æœ
            for result in results:
                if result['has_annotations']:
                    annotated_job_count += 1
                    # å°†è¿™ä¸ªjobçš„æ‰€æœ‰å¸§æ ‡è®°ä¸ºå·²æ ‡æ³¨
                    for frame_idx in range(result['start_frame'], result['stop_frame'] + 1):
                        if frame_idx < len(image_basenames):
                            cvat_annotated_images.add(image_basenames[frame_idx])
            
            logger.info(f"   âœ“ å·²æ ‡æ³¨jobs: {annotated_job_count}/{len(jobs)}")
        
        logger.info(f"\nâœ… CVATç»Ÿè®¡:")
        logger.info(f"   å·²åŠ è½½å›¾ç‰‡: {len(cvat_images)} ä¸ª")
        logger.info(f"   å·²æ ‡æ³¨å›¾ç‰‡: {len(cvat_annotated_images)} ä¸ª")
    
    # 5. å¯¹æ¯”åˆ†æ
    logger.info(f"\nğŸ” åˆ†æç»“æœ...")
    
    loaded_not_annotated = cvat_images - cvat_annotated_images
    
    if cloud_basenames is not None:
        # æœ‰äº‘å­˜å‚¨æ•°æ®ï¼Œè¿›è¡Œå¯¹æ¯”
        new_images = cloud_basenames - cvat_images
        
        logger.info(f"\nğŸ“Š å¯¹æ¯”ç»“æœ:")
        logger.info(f"   äº‘å­˜å‚¨æ€»æ–‡ä»¶: {len(cloud_basenames)}")
        logger.info(f"   å·²åŠ è½½åˆ°CVAT: {len(cvat_images)}")
        logger.info(f"   CVATå·²æ ‡æ³¨: {len(cvat_annotated_images)}")
        logger.info(f"   å·²åŠ è½½æœªæ ‡æ³¨: {len(loaded_not_annotated)}")
        logger.info(f"   æœªåŠ è½½ï¼ˆæ–°æ•°æ®ï¼‰: {len(new_images)}")
        
        result = {
            'summary': {
                'cloud_total': len(cloud_basenames),
                'cvat_loaded': len(cvat_images),
                'cvat_annotated': len(cvat_annotated_images),
                'cvat_not_annotated': len(loaded_not_annotated),
                'new_images': len(new_images),
            },
            'new_images': sorted(list(new_images)),
            'annotated_images': sorted(list(cvat_annotated_images)),
            'not_annotated_images': sorted(list(loaded_not_annotated)),
        }
        
        # ç”Ÿæˆæ–°æ•°æ®æ–‡ä»¶åˆ—è¡¨
        if new_images:
            new_images_file = log_dir / f'new_images_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
            with open(new_images_file, 'w', encoding='utf-8') as f:
                for img in sorted(new_images):
                    # ä½¿ç”¨å®Œæ•´çš„äº‘å­˜å‚¨è·¯å¾„
                    full_path = cloud_path_map.get(img, f"{prefix}{img}")
                    f.write(f"{full_path}\n")
            
            logger.info(f"\nâœ… æ–°æ•°æ®åˆ—è¡¨å·²ä¿å­˜: {new_images_file}")
            logger.info(f"ğŸ’¡ ä¸‹ä¸€æ­¥: ä½¿ç”¨ import_new_data.py å¯¼å…¥æ–°æ•°æ®")
    else:
        # åªæœ‰CVATæ•°æ®
        logger.info(f"\nğŸ“Š CVATæ ‡æ³¨çŠ¶æ€:")
        logger.info(f"   æ€»å›¾ç‰‡æ•°: {len(cvat_images)}")
        logger.info(f"   å·²æ ‡æ³¨: {len(cvat_annotated_images)}")
        logger.info(f"   æœªæ ‡æ³¨: {len(loaded_not_annotated)}")
        
        result = {
            'summary': {
                'cvat_total': len(cvat_images),
                'cvat_annotated': len(cvat_annotated_images),
                'cvat_not_annotated': len(loaded_not_annotated),
            },
            'annotated_images': sorted(list(cvat_annotated_images)),
            'not_annotated_images': sorted(list(loaded_not_annotated)),
        }
    
    # 6. ä¿å­˜ç»“æœ
    result_file = log_dir / f'annotation_status_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nâœ… ç»“æœå·²ä¿å­˜: {result_file}")
    
    logger.info(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info("="*60)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import sys
    
    task_ids = None
    if len(sys.argv) > 1:
        # æ”¯æŒæŒ‡å®šä»»åŠ¡ID
        try:
            task_ids = [int(tid) for tid in sys.argv[1:]]
            logger.info(f"æ£€æŸ¥æŒ‡å®šä»»åŠ¡: {task_ids}")
        except ValueError:
            logger.error("âŒ ä»»åŠ¡IDå¿…é¡»æ˜¯æ•°å­—")
            logger.info("ç”¨æ³•: python check_annotation_status.py [task_id1] [task_id2] ...")
            return
    
    check_annotation_status(task_ids=task_ids)


if __name__ == "__main__":
    main()
